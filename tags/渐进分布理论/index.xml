<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>渐进分布理论 on 嘉树的世界</title><link>https://gaxu.xyz/tags/%E6%B8%90%E8%BF%9B%E5%88%86%E5%B8%83%E7%90%86%E8%AE%BA/</link><description>Recent content in 渐进分布理论 on 嘉树的世界</description><generator>Hugo</generator><language>zh</language><copyright>2023–2024 嘉树.</copyright><lastBuildDate>Mon, 01 May 2023 20:37:45 +0800</lastBuildDate><atom:link href="https://gaxu.xyz/tags/%E6%B8%90%E8%BF%9B%E5%88%86%E5%B8%83%E7%90%86%E8%AE%BA/index.xml" rel="self" type="application/rss+xml"/><item><title>Ridge Regression</title><link>https://gaxu.xyz/posts/ridge-regression/</link><pubDate>Fri, 17 Mar 2023 13:37:45 +0800</pubDate><guid>https://gaxu.xyz/posts/ridge-regression/</guid><description>内容提要 一周以前（3月7日）考计量期中，第一题是证明岭回归估计量的渐进分布的，我没有完整地写出来，这里面还是有些技巧，之前我并不熟悉，现在总结在这里。
岭回归简介 Ridge regression，中文译名「岭回归」，是一种利用 正则化方法 处理 不适定问题 的技术。我们知道，使用 OLS 时是要求自变量之间不存在完全多重共线性的，即回归元矩阵必须是列满秩的，否则最小二乘问题没有唯一解；但如果某些变量间的相关性很高，甚至接近完全相关，OLS 估计量的方差将会相当大，估计精度大大下降。这个问题在待估参数很多时常常发生。岭回归通过对系数向量的 $L^2$ 范数（的平方）施加一个惩罚，控制系数的整体大小，达到以一定程度的偏误换取更低方差的目的（见 bias-variance tradeoff）。
Formulation 考虑线性模型：$y_i = \bm{x}_i^\prime\bm{\beta} + \varepsilon_i$，$i=1,\dots,N$。其中 $\bm{x}_i$ 和 $\bm{\beta}$ 都是 $p$ 维向量。我们更喜欢表示为矩阵形式：
$$ \bm{Y} = \bm{X}\bm{\beta} + \bm{\mathcal{E}} $$
其中 $\bm{Y} = (y_1,\dots,y_N)^\prime$，$\bm{X} = (\bm{x}_1,\dots,\bm{x}_N)^\prime$，$\bm{\mathcal{E}} = (\varepsilon_1,\dots,\varepsilon_N)^\prime$。假设 $\mathbb{E}(\varepsilon_i|\bm{x}_i) = 0$ 以及 $\mathbb{E}(\varepsilon_i^2|\bm{x}_i) = \sigma^2$，并假设 $\{(\bm{x}_i,y_i)\}_{i=1}^n$ 是独立同分布的。
岭回归估计量 $\hat{\bm{\beta}}$ 来自如下带有惩罚项的最小二乘问题：
$$ \min_{\bm{\beta}}\quad \frac1n(\bm{Y}-\bm{X}\bm{\beta})^\prime(\bm{Y}-\bm{X}\bm{\beta}) + \lambda\bm{\beta}^\prime\bm{\beta} \tag{$*$} $$
这里 $\lambda$ 是一个参数，它衡量了惩罚的大小。当 $\lambda=0$ 时，该问题就是 OLS；当 $\lambda\to\infty$ 时，$\bm{\beta} = \bm{0}$。事实上，这个问题等价于一个有约束的最小二乘：</description></item></channel></rss>
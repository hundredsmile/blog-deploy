<!doctype html><html lang=en-US><head><title>Bayesian Persuasion</title>
<meta charset=utf-8><meta name=X-UA-Compatible content="IE=edge"><meta name=google-site-verification content><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name=viewport><meta content="telephone=no" name=format-detection><meta name=description content><meta name=renderer content="webkit"><meta name=theme-color content="#ffffff"><meta property="og:url" content="https://gaxu.xyz/posts/bayesian-persuasion/"><meta property="og:site_name" content="嘉树的世界"><meta property="og:title" content="Bayesian Persuasion"><meta property="og:description" content="内容提要 本文是基于 Kamenica and Gentzkow (2011) 的学习笔记，仅对微观课上覆盖的内容做了分析，目的只是通过考试，并不是对文章的全面剖析，但由于上课时一头雾水，为了彻底理解均衡的概念，有必要深入学习。以下分为两节，第一节直接介绍模型框架和重要推论，第二节给出解题的一般方法，此方法是几何直观的，依赖于第一节的分析。
相比于普通的信号博弈，Bayesian persuasion 中的信号发送者表现得既像 informed，又像 uninformed。其 informed 的地方在于，发送者所发送的信号是条件于 state 的；其 uninformed 的地方在于，发送者最大化事前的期望效用（无条件期望）。此外，信号结构并非是给定的，而全由发送者决定，即内生化了。
如前所示，由于信号发送者全权决定信号结构，他可以将信号设计得非常复杂，为简化分析，我们作出一个重要假设：发送者必须如实地告知接收者信号的实现结果。这基本上是在假设信号只有一层。
一般模型框架 信号发送者（简称「发送者」）所发送的具体内容是什么并不重要，重要的是，其发送的信号是一个（条件）分布，条件于 state，state 不同则分布不同，这种分布上的差异传递出了关于 state 的信息，迫使一个（贝叶斯式的）信号接收者（简称「接收者」）更新自己的信念，反而采取有利于发送者的行动。
模型设定 基本的模型设定如下：
State space $\Omega$ 是一个有限集； 接收者的 action space $A$ 是一个紧集； 发送者有定义在 $A\times\Omega$ 上的（关于 $a$）连续效用函数 $v(a,\omega)$； 接收者有定义在 $A\times\Omega$ 上的（关于 $a$）连续效用函数 $u(a,\omega)$； 发送者和接收者具有共同的关于 state 的先验信念 $\mu_0$，它是 $\Omega$ 上的概率测度，且严格为正（为了应用 Bayes 法则）； 一个信号 $\pi$ 是条件于 $\omega$ 的概率测度的集合：$\{\pi(\cdot|\omega)\}_{\omega\in\Omega}$，每个概率测度都定义在同一 realization space $S$ 上，$S$ 由发送者内生决定。 以概率论的视角来看，由于我们引入了条件概率 $\pi(s|\omega)$，$(\omega,s)$ 将有 $\Omega\times S$ 上的联合分布。"><meta property="og:locale" content="zh"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-08T22:25:46+08:00"><meta property="article:modified_time" content="2023-04-11T00:02:42+08:00"><meta property="article:tag" content="博弈论"><meta name=twitter:card content="summary"><meta name=twitter:title content="Bayesian Persuasion"><meta name=twitter:description content="内容提要 本文是基于 Kamenica and Gentzkow (2011) 的学习笔记，仅对微观课上覆盖的内容做了分析，目的只是通过考试，并不是对文章的全面剖析，但由于上课时一头雾水，为了彻底理解均衡的概念，有必要深入学习。以下分为两节，第一节直接介绍模型框架和重要推论，第二节给出解题的一般方法，此方法是几何直观的，依赖于第一节的分析。
相比于普通的信号博弈，Bayesian persuasion 中的信号发送者表现得既像 informed，又像 uninformed。其 informed 的地方在于，发送者所发送的信号是条件于 state 的；其 uninformed 的地方在于，发送者最大化事前的期望效用（无条件期望）。此外，信号结构并非是给定的，而全由发送者决定，即内生化了。
如前所示，由于信号发送者全权决定信号结构，他可以将信号设计得非常复杂，为简化分析，我们作出一个重要假设：发送者必须如实地告知接收者信号的实现结果。这基本上是在假设信号只有一层。
一般模型框架 信号发送者（简称「发送者」）所发送的具体内容是什么并不重要，重要的是，其发送的信号是一个（条件）分布，条件于 state，state 不同则分布不同，这种分布上的差异传递出了关于 state 的信息，迫使一个（贝叶斯式的）信号接收者（简称「接收者」）更新自己的信念，反而采取有利于发送者的行动。
模型设定 基本的模型设定如下：
State space $\Omega$ 是一个有限集； 接收者的 action space $A$ 是一个紧集； 发送者有定义在 $A\times\Omega$ 上的（关于 $a$）连续效用函数 $v(a,\omega)$； 接收者有定义在 $A\times\Omega$ 上的（关于 $a$）连续效用函数 $u(a,\omega)$； 发送者和接收者具有共同的关于 state 的先验信念 $\mu_0$，它是 $\Omega$ 上的概率测度，且严格为正（为了应用 Bayes 法则）； 一个信号 $\pi$ 是条件于 $\omega$ 的概率测度的集合：$\{\pi(\cdot|\omega)\}_{\omega\in\Omega}$，每个概率测度都定义在同一 realization space $S$ 上，$S$ 由发送者内生决定。 以概率论的视角来看，由于我们引入了条件概率 $\pi(s|\omega)$，$(\omega,s)$ 将有 $\Omega\times S$ 上的联合分布。"><script async src="https://www.googletagmanager.com/gtag/js?id=G-LER46KQN8K"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LER46KQN8K")}</script><script src=/js/toc.js></script><link type=text/css rel=stylesheet href=/vendor/css/bootstrap.min.css><link rel=stylesheet href=/scss/journal.min.5c2933544a8f415fedcdfe99cbe482e02e16d05ccf7094eca66d91fd6fcb7917.css integrity="sha256-XCkzVEqPQV/tzf6Zy+SC4C4W0FzPcJTspm2R/W/LeRc=" media=screen><link rel=stylesheet href=/scss/dark-mode.min.4a5455fe5612c2321a4b36920ca43ac63fa0b7f21ad7d11dbed36f73f5668449.css integrity="sha256-SlRV/lYSwjIaSzaSDKQ6xj+gt/Ia19EdvtNvc/VmhEk=" media=screen><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lora:500,500italic,700,700italic|Roboto:400,400italic,700,700italic|Montserrat:500,500italic,700,700italic|Fira+Mono:500,700|Noto+Serif+TC:500,700|Noto+Serif+SC:500,700|Material+Icons&display=swap"><script>console.log("Hello from 'layouts/partials/extended_head.html'")</script><link rel=stylesheet href=/vendor/css/lxgwwenkaiscreenr.css><script defer src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/js/all.min.js></script><link rel=stylesheet href=/scss/custom/_custom.min.07270570cc4a8650f6229d55a585534fa1253613904e2c8ddf06558255549241.css integrity="sha256-BycFcMxKhlD2Ip1VpYVTT6ElNhOQTiyN3wZVglVUkkE=" media=screen><link rel=stylesheet href=/vendor/css/heti.min.css><script src=/vendor/js/heti.min.js></script><script defer>function domReady(){var e=document.querySelector(".post-subtitle");e&&e.classList.add("heti"),e=document.querySelector("#post-content"),e&&e.classList.add("heti"),document.querySelectorAll(".post-item-summary").forEach(e=>{e.classList.add("heti")})}document.addEventListener("DOMContentLoaded",function(){document.removeEventListener("DOMContentLoaded",arguments.callee,!1),domReady()});const heti=new Heti(".heti");heti.autoSpacing(),window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],macros:{bm:["\\boldsymbol{#1}",1]},processEscapes:!0,processEnvironments:!0},svg:{scale:.962,fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],enableMenu:!1}},function(){var e=document.createElement("script");e.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js",e.async=!0,document.addEventListener("DOMContentLoaded",function(){document.head.appendChild(e)})}()</script><script>document.addEventListener("DOMContentLoaded",function(){var e,t=document.getElementsByClassName("collapsible");for(e=0;e<t.length;e++)t[e].addEventListener("click",function(){this.classList.toggle("active");var e=this.nextElementSibling;e.style.display==="block"?e.style.display="none":e.style.display="block"})})</script></head><body><div id=app><div class=single-column-drawer-container id=drawer v-bind:class="{ 'single-column-drawer-container-active': isDrawerOpen }"><div class=drawer-content><div class=drawer-menu><a class="a-block drawer-menu-item false" href=/about>关于我
</a><a class="a-block drawer-menu-item active" href=/posts>归档
</a><a class="a-block drawer-menu-item false" href=/categories>分类
</a><a class="a-block drawer-menu-item false" href=/tags>标签
</a><a class="a-block drawer-menu-item false" href=/index.xml>订阅</a><div class=toc><div class=toc-content><center>目录</center><ul><ul><li><a href=#%e5%86%85%e5%ae%b9%e6%8f%90%e8%a6%81 class=nav-内容提要>内容提要</a></li><li><a href=#%e4%b8%80%e8%88%ac%e6%a8%a1%e5%9e%8b%e6%a1%86%e6%9e%b6 class=nav-一般模型框架>一般模型框架</a></li><ul><li><a href=#%e6%a8%a1%e5%9e%8b%e8%ae%be%e5%ae%9a class=nav-模型设定>模型设定</a></li><li><a href=#timing class=nav-timing>Timing</a></li><li><a href=#%e5%9d%87%e8%a1%a1%e7%9a%84%e5%ae%9a%e4%b9%89 class=nav-均衡的定义>均衡的定义</a></li><li><a href=#%e5%af%b9%e5%8f%91%e9%80%81%e8%80%85%e6%9c%80%e4%bc%98%e5%8c%96%e9%97%ae%e9%a2%98%e7%9a%84%e5%8f%98%e6%8d%a2 class=nav-对发送者最优化问题的变换>对发送者最优化问题的变换</a></li><li><a href=#%e6%8a%80%e6%9c%af%e8%a1%a5%e5%85%85 class=nav-技术补充>技术补充$^*$</a></li></ul><li><a href=#%e4%b8%80%e8%88%ac%e9%97%ae%e9%a2%98%e4%b8%ad%e7%9a%84%e6%b1%82%e8%a7%a3%e6%96%b9%e5%bc%8f class=nav-一般问题中的求解方式>一般问题中的求解方式</a></li><ul><li><a href=#%e4%b8%80%e4%b8%aa%e4%be%8b%e5%ad%90 class=nav-一个例子>一个例子</a></li></ul><li><a href=#%e5%8f%82%e8%80%83 class=nav-参考>参考</a></li></ul></div></div></div></div></div><transition name=fade><div id=drawer-mask v-bind:class="{ 'single-column-drawer-mask': mounted }" v-if=isDrawerOpen v-on:click=toggleDrawer></div></transition><nav id=navBar class="navbar sticky-top navbar-light single-column-nav-container"><div id=navBackground class=nav-background></div><div class="container container-narrow nav-content"><button id=nav_dropdown_btn class=nav-dropdown-toggle type=button v-on:click=toggleDrawer>
<i class=material-icons>menu
</i></button>
<a id=navTitle class=navbar-brand href=https://gaxu.xyz/>嘉树的世界
</a><button type=button class=nav-darkmode-toggle id=darkModeToggleButton2>
<i class=material-icons id=darkModeToggleIcon2>dark_mode</i></button></div></nav><div class=single-column-header-container id=pageHead v-bind:style="{ transform: 'translateZ(0px) translateY('+.3*scrollY+'px)', opacity: 1-navOpacity }"><a href=https://gaxu.xyz/><div class=single-column-header-title>嘉树的世界</div><div class=single-column-header-subtitle>靡不有初，鲜克有终</div></a></div><div id=content><div id=streamContainer class=stream-container><div class="post-list-container post-list-container-shadow"><div class=post><div class=post-head-wrapper-text-only><div class=post-title>Bayesian Persuasion<div class=post-meta>嘉树 &nbsp;
<time itemprop=datePublished>2023-03-08 22:25
</time>&emsp;
<i class=material-icons>folder</i>
<a href=/categories/%E7%BB%8F%E6%B5%8E%E5%AD%A6>经济学</a>
&nbsp;
<i class=material-icons>label</i>
<a href=/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA>博弈论</a>
&nbsp;</div></div></div><div class=post-body-wrapper><div class=post-body v-pre><div id=post-content><h2 id=内容提要>内容提要&nbsp;<a class=hash-link href=#内容提要><i class="fas fa-link"></i></a></h2><p>本文是基于 Kamenica and Gentzkow (2011) 的学习笔记，仅对微观课上覆盖的内容做了分析，目的只是通过考试，并不是对文章的全面剖析，但由于上课时一头雾水，为了彻底理解均衡的概念，有必要深入学习。以下分为两节，第一节直接介绍模型框架和重要推论，第二节给出解题的一般方法，此方法是几何直观的，依赖于第一节的分析。</p><p>相比于普通的信号博弈，Bayesian persuasion 中的信号发送者表现得既像 informed，又像 uninformed。其 informed 的地方在于，发送者所发送的信号是条件于 state 的；其 uninformed 的地方在于，发送者最大化事前的期望效用（无条件期望）。此外，信号结构并非是给定的，而全由发送者决定，即内生化了。</p><p>如前所示，由于信号发送者全权决定信号结构，他可以将信号设计得非常复杂，为简化分析，我们作出一个重要假设：发送者必须如实地告知接收者信号的实现结果。这基本上是在假设信号只有一层。</p><h2 id=一般模型框架>一般模型框架&nbsp;<a class=hash-link href=#一般模型框架><i class="fas fa-link"></i></a></h2><blockquote><p>信号发送者（简称「发送者」）所发送的具体内容是什么并不重要，重要的是，其发送的信号是一个（条件）分布，条件于 state，state 不同则分布不同，这种分布上的差异传递出了关于 state 的信息，迫使一个（贝叶斯式的）信号接收者（简称「接收者」）更新自己的信念，反而采取有利于发送者的行动。</p></blockquote><h3 id=模型设定>模型设定&nbsp;<a class=hash-link href=#模型设定><i class="fas fa-link"></i></a></h3><p>基本的模型设定如下：</p><ul><li>State space $\Omega$ 是一个有限集；</li><li>接收者的 action space $A$ 是一个紧集；</li><li>发送者有定义在 $A\times\Omega$ 上的（关于 $a$）连续效用函数 $v(a,\omega)$；</li><li>接收者有定义在 $A\times\Omega$ 上的（关于 $a$）连续效用函数 $u(a,\omega)$；</li><li>发送者和接收者具有共同的关于 state 的先验信念 $\mu_0$，它是 $\Omega$ 上的概率测度，且严格为正（为了应用 Bayes 法则）；</li><li>一个信号 $\pi$ 是条件于 $\omega$ 的概率测度的集合：$\{\pi(\cdot|\omega)\}_{\omega\in\Omega}$，每个概率测度都定义在同一 realization space $S$ 上，$S$ 由发送者内生决定。</li></ul><p>以概率论的视角来看，由于我们引入了条件概率 $\pi(s|\omega)$，$(\omega,s)$ 将有 $\Omega\times S$ 上的联合分布。</p><h3 id=timing>Timing&nbsp;<a class=hash-link href=#timing><i class="fas fa-link"></i></a></h3><p>博弈的 timing 如下：</p><ol><li>Nature 从分布 $\mu_0$ 中选择一个 state；</li><li>发送者选择一个信号；</li><li>接收者观察到信号结构和信号的 realization，选择一个行动。</li></ol><p>整个 timing 和普通的信号模型本质上并无不同，本质上不同的还是信号结构的内生以及发送者 informed 的程度。</p><h3 id=均衡的定义>均衡的定义&nbsp;<a class=hash-link href=#均衡的定义><i class="fas fa-link"></i></a></h3><p>称 $(\pi,\hat{a}(\mu_s))$ 为一个<strong>有利于发送者的子博弈完美均衡</strong>（sender-preferred subgame perfect equilibrium）若</p><ol><li>给定 $\pi$ 和每一个 realization $s$，接收者按照 Bayes 法则形成后验 $\mu(\omega|s)$，简记为 $\mu_s$，$\hat{a}(\mu_s)$ 最大化了接收者的后验期望效用：</li></ol><p>$$\hat{a}(\mu_s)\in\arg\max_{a\in A}\mathbb{E}_{\mu_s}[u(a,\omega)]$$</p><ol start=2><li>给定 $\hat{a}(\mu_s)$，$\pi$ 最大化了发送者事前期望效用（无条件期望）$\mathbb{E}[v(\hat{a}(\mu_s),\omega)]$。</li></ol><style type=text/css>.notice{--root-color:#444;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#c33;--warning-content:#fee;--info-title:#fb7;--info-content:#fec;--note-title:#6be;--note-content:#e7f2fa;--tip-title:#5a5;--tip-content:#efe}@media(prefers-color-scheme:dark){.notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}}body.night .notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}.notice{padding:0 18px 18px;line-height:24px;margin-bottom:20px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:var(--title-color)}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative}</style><div><svg width="0" height="0" display="none"><symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379.0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628.0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628.0l-22.627 22.627c-6.248 6.248-6.248 16.379.0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"/></symbol><symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405.0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346 7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373.0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884.0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet"><path d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937.0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154.0l239.94 416.028zM288 354c-25.405.0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346 7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373.0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884.0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196.0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627.0 12 5.373 12 12v1e2h12c6.627.0 12 5.373 12 12v24z"/></symbol></svg></div><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>Note</p><p>这里最重要的设定是发送者最大化<strong>事前</strong>无条件期望，而不是像一般的发信号模型那样，每个 type 的发送者分别最大化自己的期望效用（即条件于 type）。但信号又条件于 state，此意义上讲，发送者似有多个信息集。这也是为什么我在本文开头说，此模型中的信号发送者表现得既像 informed，又像 uninformed。对此，我的解释是，发送者所设计的信号是 state-contingent，当 state 实现，这个信号就会坍缩为相应的子信号，也就是条件于 state 的一个分布。</p></div><p>为方便起见，定义信号的<strong>价值</strong>为上述无条件期望。定义信号的<strong>增益</strong>为其价值和当接收者没有获得任何信息情况下 $v(a,\omega)$ 的期望（即 $\mathbb{E}[v(\hat{a}(\mu_0),\omega)]$）之间的差值（简单来说就是一个信号相比无信息信号能带来多大的期望效用增益）。如果增益严格为正，那么我们说接收者从 persuation 中获益（benefits from persuasion）。最优信号指的是实现了最大增益的信号。显然，在均衡中发送者会选择最优信号。</p><p>无条件期望可以根据迭代期望法则写为：$\mathbb{E}[\mathbb{E}[v(\hat{a}(\mu_s),\omega)|\omega]]$，其中内层的条件期望即是在 $\pi(s|\omega)$ 下取得，外层的期望则是在 $\mu_0(\omega)$ 下取得。直接求解最优的条件分布无疑是很困难的，主要有两个原因：第一，realization space 的选择是内生的而非给定的；第二，即便我们能缩小 realization space 的选择范围，也很难有一种直观上的方式求解 $\pi$。如果我们能转换思路，先求最优后验信念 $\mu(\omega|s)$，再恢复 $\pi$，事情将好办很多。这就是下一小节的内容。</p><h3 id=对发送者最优化问题的变换>对发送者最优化问题的变换&nbsp;<a class=hash-link href=#对发送者最优化问题的变换><i class="fas fa-link"></i></a></h3><blockquote><p><em>We can reexpress the problem of choosing an optimal signal as a search over distributions of posteriors subject to the constraint that the expected posterior is equal to the prior.</em></p></blockquote><p>简单来说，我们先条件于 $s$ 而不是 $\omega$，即</p><p>$$
\mathbb{E}[v(\hat{a}(\mu_s),\omega)] = \mathbb{E}[\mathbb{E}[v(\hat{a}(\mu_s),\omega)|s]]
$$</p><p>内层的条件期望在后验信念 $\mu(\omega|s)$ 下取得。发送者寻找最优信号 $\pi(s|\omega)$ 的目标等价于寻找最优后验信念 $\mu(\omega|s)$ 和 $s$ 的无条件分布 $\tau(s)$，随后可由 Bayes 法则恢复信号：</p><p>$$
\pi(s|\omega) = \mu(\omega|s)\tau(s)/\mu_0(\omega)
$$</p><p>这也必定保证了接收者按照 Bayes 法则更新自己的信念。当然，为保证概率和为 1，即 $\sum_s\pi(s|\omega) = 1$，我们要求 $\mu(\omega|s)$ 和 $\tau(s)$ 满足 $\int_s \mu(\omega|s)\thinspace d\tau(s) = \mu_0(\omega)$，这在文中被称为 <em><strong>Bayes plausibility</strong></em>。</p><p>不过，我们希望更进一步地简化问题，可能有一些 $s$，它们会导致相同的后验信念，这将意味着条件期望 $\mathbb{E}[v(\hat{a}(\mu_s),\omega)|s]$ 在这些 $s$ 间也是相同的，我们可以将这些 $s$ 视为同一类。这样，不同类可由它们导致的后验 $\mu$ 进行区分，从而得到 $\mu$ 的一个集合，我们再在此上定义概率空间。于是 $(\omega,s)$ 联合分布变成 $(\omega,\mu)$ 联合分布。我们仍用 $\tau$ 表示 $\mu$ 的无条件分布（或边缘分布）。具体而言，我们有</p><p>$$\tau(\mu)=\sum_{\{s\colon\mu_s=\mu\}}\tau(s)=\sum_{\{s\colon\mu_s=\mu\}}\sum_{\omega}\pi(s|\omega)\mu_0(\omega)$$</p><p>当然，如果 $\mu_s$ 和 $s$ 一一对应的话，我们只是换了一下记号而已。</p><p>这种整合并未改变无条件期望，即</p><p>$$
\mathbb{E}[v(\hat{a}(\mu),\omega)] = \mathbb{E}[\mathbb{E}[v(\hat{a}(\mu),\omega)|\mu]] = \mathbb{E}_\tau[\mathbb{E}_\mu[v(\hat{a}(\mu),\omega)]]
$$</p><p>我们将内层的条件期望记作 $\hat{v}(\mu)$，因此上述无条件期望就是 $\mathbb{E}_{\tau}\hat{v}(\mu)$。Bayes plausibility 要求 $\int_{\mu}\mu\thinspace d\tau(\mu) = \mu_0$。</p><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>Note</p><p>以上的变换本质上是在压缩 realization space，似乎不是很有必要。我们可以假设每个 state 引致的后验都是不同的，即假设 $\mu_s$ 和 $s$ 一一对应。</p></div><p>以上的分析已经足够我们理解本文的核心论断，即下一小节的 <a href=#thm%3abestsig>推论</a>，只是为了严谨，尚需处理一些细节问题。</p><h3 id=技术补充>技术补充$^*$&nbsp;<a class=hash-link href=#技术补充><i class="fas fa-link"></i></a></h3><p>现在有两个问题，第一，发送者可以选择十分复杂的信号结构，换言之，信号的 realization space 可能大于接收者的 action space。这种情况实际上可以得到简化，我们最终只需考虑和 action space 形成一一对应的 realization space。我们将 realization space 的势不大于 action space 的势的那些信号称为<strong>简单信号</strong>（straightforward signal<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>）。第二个问题是，给定满足 Bayes plausibility 的 $\mu$ 和 $\tau$，其背后是否一定有 well-defined 之信号 $\pi$？尽管 Bayes 法则能用于离散和连续情况，但我们希望对于一般的情况进行证明。</p><p>我们可以证明如下命题：</p><div class=thm><p id=thm:equiv class="thm-title thm-proposition">Proposition 1.</p><div class="thm-inner thm-proposition"><p>以下表述等价：</p><ol><li>存在某个信号，其价值为 $v^*$；</li><li>存在简单信号，其价值为 $v^*$;</li><li>存在 Bayes-plausible 分布 $\tau$，使得 $\mathbb{E}_{\tau}\hat{v}(\mu) = v^*$。</li></ol></div></div><div class=thm><button type=button class=collapsible><div id=thm-proof-1 class=thm-title-proof>Proof.</div></button><div class=proof-content><div class=thm-inner-proof><p>根据定义，易知命题 2 可导出命题 1 和 3。现在证明命题 1 可导出命题 2。对于一个信号 $\pi$，假设其价值为 $v^*$，令 $S^a = \{s\colon\hat{a}(\mu_s)=a\}$，也就是说，我们把使得接收者的最优行动为 $a$ 的信号 realization 都收集起来构成一个集合。我们可以据此构造一个新的信号：其 realization space 和接收者的 action space一一对应，即 $S^\prime \leftrightarrow A$，定义 $\pi^\prime(\alpha|\omega) = \sum_{s\in S^a}\pi(s|\omega)$。简单来说，我们对原来的 realization space 按照其引致的 action 做了一个分割，然后将每个子集上的概率加总得到一个新的概率，新的 realization space 则和 action space 构成一一映射（$\alpha$ 和 $a$）。因为 $a$ 对每个 $s\in S^a$ 都是接收者的最优行动，对于 $\alpha\in S^\prime$，$a$ 也必定是最优行动。如果要严格证明这一点，只需注意到 $\mu(\omega|\alpha) \propto \pi(\alpha|\omega)\mu_0(\omega) = \sum_{s\in S^a}\pi(s|\omega)\mu_0(\omega)$，而 $\mathbb{E}_{\mu_s}[u(a,\omega)] \propto \sum_{\omega\in\Omega}\pi(s|\omega)\mu_0(\omega)u(a,\omega)$。这种构造意味着，条件于 $\omega$，最优行动在 $\pi^\prime$ 下的分布和在 $\pi$ 下的分布相同；具体而言，在 $\pi^\prime$ 下，最优行动取 $a$ 的条件概率为 $\pi(\alpha|\omega)$，在 $\pi$ 下，取 $a$ 的条件概率为 $\sum_{s\in S^a}\pi(s|\omega) = \pi(\alpha|\omega)$。这意味着 $\mathbb{E}[v(\hat{a}_s,\omega)|\omega] = \mathbb{E}[v(\hat{a}_\alpha,\omega)|\omega]$，从而其无条件期望相等，也就是两种信号的价值都为 $v^*$。</p><p>现在证明命题 3 可导出命题 1。注意，我们不能直接根据 $\tau$ 来构造信号，因为 $\tau$ 的支撑集可能是无限集，从而构造的信号为零概率（似乎可以将 $\tau$ 视为概率密度来构造信号的密度，不过这并不能涵盖任意分布）。由于 $\Omega$ 是有限集（假设有 $d$ 个元素），这意味着 $\hat{v}(\mu)$ 可视作 $(\mu_s(\omega_1),\dots,\mu_s(\omega_{d-1}))$ 的函数。于是，我们得到了 $\mathbb{R}^d$ 空间内的一系列点：$(\mu_s(\omega_1),\dots,\mu_s(\omega_{d-1}),\hat{v})\in\mathbb{R}^d$。这样一来，$\mathbb{E}_{\tau}\hat{v}(\mu) = v^*$ 也是 $\mathbb{R}^d$ 空间内的一点：$(\mu_0(\omega_1),\dots,\mu_0(\omega_{d-1}),v^*)$，更重要的是，它位于前述那些点构成的 <a href=https://en.wikipedia.org/wiki/Convex_hull>凸包络集（或凸包）</a> 之内。因此，应用 <a href=https://en.wikipedia.org/wiki/Carath%C3%A9odory%27s_theorem_%28convex_hull%29>Carathéodory 定理</a>，它一定可以表述为该凸包络集内<strong>有限</strong>个点的凸组合，换言之，存在一个支撑集为<strong>有限</strong>集的 Bayes-plausible 分布 $\tau^*$，使得 $\mathbb{E}_{\tau^*}\hat{v}(\mu) = v^*$。据此，我们可以定义信号的 realization space 为 $\{s\colon\mu_s\in\text{Supp}(\tau^*)\}$，进而定义信号 $\pi(s|\omega) = \mu(\omega|s)\tau^*(\mu_s)/\mu_0(\omega)$，这个信号将引致 $\tau^*$，从而具有价值 $v^*$。</p></div><div class=thm-end-proof><i class="fas fa-square"></i> <a href=#thm-proof-1><i class="fas fa-arrow-circle-up"></i></a></div></div></div><p>从以上定理我们可以绕过对最优信号的直接求解，而是先求 $\tau$ 和相应的支撑集，再按照 Bayes 法则恢复出 $\pi$，这已经被证明一定可以做到。我们立即有如下推论：</p><div class=thm><p id=thm:bestsig class="thm-title thm-corollary">Corollary 2.</p><div class="thm-inner thm-corollary"><p>发送者可以从 persuation 中获益当且仅当存在一个 Bayes-plausible 分布 $\tau$ 使得</p><p>$$\mathbb{E}_{\tau}\hat{v}(\mu) > \hat{v}(\mu_0)$$</p><p>此外，最优信号的价值就是</p><p>$$\max_{\tau}\ \mathbb{E}_{\tau}\hat{v}(\mu) \quad \text{s.t.}\int\mu\thinspace d\tau(\mu) = \mu_0$$</p></div></div><div class=thm><button type=button class=collapsible><div id=thm-proof-2 class=thm-title-proof>Proof.</div></button><div class=proof-content><div class=thm-inner-proof>显然，略。</div><div class=thm-end-proof><i class="fas fa-square"></i> <a href=#thm-proof-2><i class="fas fa-arrow-circle-up"></i></a></div></div></div><p>值得一提，根据 <a href=#thm%3aequiv>命题 1</a>，我们只需关注 realization space 和 action space 等势的情况。</p><h2 id=一般问题中的求解方式>一般问题中的求解方式&nbsp;<a class=hash-link href=#一般问题中的求解方式><i class="fas fa-link"></i></a></h2><p>在一般问题中，我们如何求解最优信号及其价值呢？事实上，<a href=#thm%3abestsig>上述推论</a> 意味着我们可以有一种几何上非常直观的方式。</p><p>假设 action space 有两个元素，那么我们只用考虑 realization space 有两个元素的情况（假设 $s_1$ 和 $s_2$）。因此，$\pi$ 和 $\tau$ 都是二项分布（再次回忆，$\pi$ 是 $s$ 的条件分布，而 $\tau$ 实质上是 $s$ 的无条件分布）。此外，假设 state space 也只有两个元素，这样，我们就可以用一个维度代表后验，例如，$\mu(\omega_1|s)$，不妨简记为 $\mu_s$。按照 <a href=#thm%3abestsig>推论</a>，信号 $\pi$ 的价值就是在 $\mu$-$\hat{v}(\mu)$ 图像上的 $(\mu_{s_1},\hat{v}(\mu_{s_1}))$ 和 $(\mu_{s_2},\hat{v}(\mu_{s_2}))$ 两点和直线 $\mu=\mu_0$ 交点的纵坐标；交点就是这两点的加权中点，权重就是 $\tau$。需要注意，当两点连线和 $\mu=\mu_0$ 无交点时，说明以这两点为支撑集的任何 $\tau$ 都不是 Bayes-plausible，也就无法实现增益；当然，给定先验，我们总能找到 Bayes-plausible 分布（在 $\mu=\mu_0$ 两边取点即可）。见下图中 Panel B。</p><figure id=fig:conc align role=group aria-describedby=caption-1d1df851cd07aac8d754ff14a72c4cf1><a href=/posts/bayesian-persuasion/images/concave-closure.png class=img-link><img src=/posts/bayesian-persuasion/images/concave-closure.png style=width:80%></a><figcaption id=caption-1d1df851cd07aac8d754ff14a72c4cf1>Concave Closure (Source: Kamenica and Gentzkow, 2011, Fig.2)</figcaption></figure><p>易知两点连线总是在 $\hat{v}(\mu)$ 图像的凸包之内，因此我们取凸包的边界函数，称为 $\hat{v}$ 的<strong>凹闭包</strong>（concave closure）：</p><p>$$V(\mu) \equiv \sup\{y\colon (\mu,y)\in\text{conv}(\hat{v})\}$$</p><p>这里 $\text{conv}(\hat{v})$ 表示 $\hat{v}$ 图像的凸包络集。$V(\mu)$ 必定是凹函数（因而称为凹闭包），并且是处处大于等于 $\hat{v}$ 的最小凹函数；它衡量了，当 state 的先验为 $\mu$ 时，发送者所能实现的最大价值，如 <a href=#fig%3aconc>上图</a> Panel C 所示。</p><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>Note</p><p>即便 action space 是无限集，我们也许只需要两个 realization，前提是它们与 $\mu = \mu_0$ 的交点位于 $V(\mu)$ 图像上。</p></div><h3 id=一个例子>一个例子&nbsp;<a class=hash-link href=#一个例子><i class="fas fa-link"></i></a></h3><p>现在用一个简单例子阐述求解过程。假设在一例案件中，检方（发送者）想要说服法官（接收者）对被告作出有罪判决，具体设定如下：</p><ul><li>有两种状态：被告是 <em>guilty</em> 或者 <em>innocent</em>，先验信念 $\mu_0(\textit{guilty}) = 0.3$；</li><li>法官（接收者）有两种行动：<em>acquit</em> 或 <em>convict</em>；</li><li>法官若作出正确判决，则法官效用为 1，否则为 0；</li><li>法官若 <em>convict</em>，则检方效用为 1，否则为 0。</li></ul><p>试问，法官应该如何设定信号以最大化自己的效用？</p><p>首先，我们求法官的最优行动 $\hat{a}(\mu)$，这里 $\mu$ 表示关于被告是 <em>guilty</em> 的信念。显然，当 $\mu &lt; 0.5$ 时，法官最优行动是 <em>acquit</em>，当 $\mu \geq 0.5$ 时是 <em>convict</em>。因此，检方的后验效用就是：当 $\mu &lt; 0.5$，$\hat{v}(\mu) = 0$；当 $\mu \geq 0.5$，$\hat{v}(\mu) = 1$，如 <a href=#fig%3aconc>上图</a> 所示。我们可以在其凹闭包上找到对应于先验 $\mu_0 = 0.3$ 的最大信号价值 $V(0.3) = 0.6$。$\tau$ 的支撑集显然是 $\{\mu_{s_1} = 0,\mu_{s_2}=0.5\}$。 随后根据 Bayes plausibility 确定 $\tau$：</p><p>$$0\cdot\tau(s_1)+0.5\cdot\tau(s_2)=0.3 \Rightarrow \tau(s_1)=0.4,\tau(s_2)=0.6$$</p><p>最后，恢复最优信号：</p><p>$$
\begin{aligned}
&\pi(s_1|\textit{guilty})=\mu(\textit{guilty}\thinspace|s_1)\tau(s_1)/\mu_0(\textit{guilty}) = 0 \\
&\pi(s_1|\textit{innocent})=\mu(\textit{innocent}\thinspace|s_1)\tau(s_1)/\mu_0(\textit{innocent}) = 4/7
\end{aligned}
$$</p><p>简单来说，分三步走：第一，确定 $\tau$ 的支撑集；第二，利用 Bayes plausibility 确定 $\tau$；第三，根据 Bayes 法则恢复最优信号。</p><h2 id=参考>参考&nbsp;<a class=hash-link href=#参考><i class="fas fa-link"></i></a></h2><ul><li>Kamenica, Emir, and Matthew Gentzkow. 2011. &ldquo;Bayesian Persuasion.&rdquo; <em>American Economic Review</em>, 101 (6): 2590-2615.</li></ul><br><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>原文定义 $S\subseteq A$ 的那些信号为简单信号，这里采用更具一般性的定义。&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div><hr width=100% id=EOF style="border-top:.6px solid #777;margin-top:2.5rem"><p style=color:#777>最后修改于 2023-04-11</p></div></div></div><nav class=post-pagination><a class=older-posts>上回<br>这是最旧的文章了。
</a><a class=newer-posts href=/posts/ridge-regression/>下回<br><em>Ridge Regression</em></a></nav><div class=post-comment-wrapper><p style=opacity:.6 align=center><small>此篇文章的评论功能已经停用。</small></p></div></div></div></div></div><div id=sideContainer class=side-container><a class="a-block nav-head false" href=https://gaxu.xyz/><div class=nav-title>嘉树的世界</div><div class=nav-subtitle>靡不有初，鲜克有终</div></a><div class=nav-link-list><a class="a-block nav-link-item false" href=/about>关于我
</a><a class="a-block nav-link-item active" href=/posts>归档
</a><a class="a-block nav-link-item false" href=/categories>分类
</a><a class="a-block nav-link-item false" href=/tags>标签
</a><a class="a-block nav-link-item false" href=/index.xml>订阅</a></div><div class=nav-footer>由 <a href=https://gohugo.io>Hugo</a> 强力驱动<br>主题改动自 <a href=https://github.com/amazingrise/hugo-theme-diary>Diary</a><br>&copy;
2023–2024 嘉树.
本站遵循 <a href=https://creativecommons.org/licenses/by-nc/4.0/>CC-BY-NC 4.0</a> 协议</div></div><div id=extraContainer class=extra-container><div class=toc-wrapper><div class=toc><div class=toc-content><center>目录</center><ul><ul><li><a href=#%e5%86%85%e5%ae%b9%e6%8f%90%e8%a6%81 class=nav-内容提要>内容提要</a></li><li><a href=#%e4%b8%80%e8%88%ac%e6%a8%a1%e5%9e%8b%e6%a1%86%e6%9e%b6 class=nav-一般模型框架>一般模型框架</a></li><ul><li><a href=#%e6%a8%a1%e5%9e%8b%e8%ae%be%e5%ae%9a class=nav-模型设定>模型设定</a></li><li><a href=#timing class=nav-timing>Timing</a></li><li><a href=#%e5%9d%87%e8%a1%a1%e7%9a%84%e5%ae%9a%e4%b9%89 class=nav-均衡的定义>均衡的定义</a></li><li><a href=#%e5%af%b9%e5%8f%91%e9%80%81%e8%80%85%e6%9c%80%e4%bc%98%e5%8c%96%e9%97%ae%e9%a2%98%e7%9a%84%e5%8f%98%e6%8d%a2 class=nav-对发送者最优化问题的变换>对发送者最优化问题的变换</a></li><li><a href=#%e6%8a%80%e6%9c%af%e8%a1%a5%e5%85%85 class=nav-技术补充>技术补充$^*$</a></li></ul><li><a href=#%e4%b8%80%e8%88%ac%e9%97%ae%e9%a2%98%e4%b8%ad%e7%9a%84%e6%b1%82%e8%a7%a3%e6%96%b9%e5%bc%8f class=nav-一般问题中的求解方式>一般问题中的求解方式</a></li><ul><li><a href=#%e4%b8%80%e4%b8%aa%e4%be%8b%e5%ad%90 class=nav-一个例子>一个例子</a></li></ul><li><a href=#%e5%8f%82%e8%80%83 class=nav-参考>参考</a></li></ul></div></div></div><div class=pagination><a id=globalBackToTop class="pagination-action animated-visibility" href=#top :class="{ invisible: scrollY == 0 }"><i class="material-icons pagination-action-icon">keyboard_arrow_up
</i></a><a type=button class=pagination-action id=darkModeToggleButton><span class="material-icons pagination-action-icon" id=darkModeToggleIcon>dark_mode</span></a></div></div><div id=single-column-footer>由 <a href=https://gohugo.io>Hugo</a> 强力驱动<br>主题改动自 <a href=https://github.com/amazingrise/hugo-theme-diary>Diary</a><br>&copy;
2023–2024 嘉树.
本站遵循 <a href=https://creativecommons.org/licenses/by-nc/4.0/>CC-BY-NC 4.0</a> 协议</div></div><script src=/js/journal.js></script></body></html>
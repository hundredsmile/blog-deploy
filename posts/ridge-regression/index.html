<!doctype html><html lang=en-US><head><title>Ridge Regression</title>
<meta charset=utf-8><meta name=X-UA-Compatible content="IE=edge"><meta name=google-site-verification content><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name=viewport><meta content="telephone=no" name=format-detection><meta name=description content><meta name=renderer content="webkit"><meta name=theme-color content="#ffffff"><meta property="og:url" content="https://gaxu.xyz/posts/ridge-regression/"><meta property="og:site_name" content="嘉树的世界"><meta property="og:title" content="Ridge Regression"><meta property="og:description" content="内容提要 一周以前（3月7日）考计量期中，第一题是证明岭回归估计量的渐进分布的，我没有完整地写出来，这里面还是有些技巧，之前我并不熟悉，现在总结在这里。
岭回归简介 Ridge regression，中文译名「岭回归」，是一种利用 正则化方法 处理 不适定问题 的技术。我们知道，使用 OLS 时是要求自变量之间不存在完全多重共线性的，即回归元矩阵必须是列满秩的，否则最小二乘问题没有唯一解；但如果某些变量间的相关性很高，甚至接近完全相关，OLS 估计量的方差将会相当大，估计精度大大下降。这个问题在待估参数很多时常常发生。岭回归通过对系数向量的 $L^2$ 范数（的平方）施加一个惩罚，控制系数的整体大小，达到以一定程度的偏误换取更低方差的目的（见 bias-variance tradeoff）。
Formulation 考虑线性模型：$y_i = \bm{x}_i^\prime\bm{\beta} + \varepsilon_i$，$i=1,\dots,N$。其中 $\bm{x}_i$ 和 $\bm{\beta}$ 都是 $p$ 维向量。我们更喜欢表示为矩阵形式：
$$ \bm{Y} = \bm{X}\bm{\beta} + \bm{\mathcal{E}} $$
其中 $\bm{Y} = (y_1,\dots,y_N)^\prime$，$\bm{X} = (\bm{x}_1,\dots,\bm{x}_N)^\prime$，$\bm{\mathcal{E}} = (\varepsilon_1,\dots,\varepsilon_N)^\prime$。假设 $\mathbb{E}(\varepsilon_i|\bm{x}_i) = 0$ 以及 $\mathbb{E}(\varepsilon_i^2|\bm{x}_i) = \sigma^2$，并假设 $\{(\bm{x}_i,y_i)\}_{i=1}^n$ 是独立同分布的。
岭回归估计量 $\hat{\bm{\beta}}$ 来自如下带有惩罚项的最小二乘问题：
$$ \min_{\bm{\beta}}\quad \frac1n(\bm{Y}-\bm{X}\bm{\beta})^\prime(\bm{Y}-\bm{X}\bm{\beta}) + \lambda\bm{\beta}^\prime\bm{\beta} \tag{$*$} $$"><meta property="og:locale" content="zh"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-17T13:37:45+08:00"><meta property="article:modified_time" content="2023-05-01T20:37:45+08:00"><meta property="article:tag" content="渐进分布理论"><meta name=twitter:card content="summary"><meta name=twitter:title content="Ridge Regression"><meta name=twitter:description content="内容提要 一周以前（3月7日）考计量期中，第一题是证明岭回归估计量的渐进分布的，我没有完整地写出来，这里面还是有些技巧，之前我并不熟悉，现在总结在这里。
岭回归简介 Ridge regression，中文译名「岭回归」，是一种利用 正则化方法 处理 不适定问题 的技术。我们知道，使用 OLS 时是要求自变量之间不存在完全多重共线性的，即回归元矩阵必须是列满秩的，否则最小二乘问题没有唯一解；但如果某些变量间的相关性很高，甚至接近完全相关，OLS 估计量的方差将会相当大，估计精度大大下降。这个问题在待估参数很多时常常发生。岭回归通过对系数向量的 $L^2$ 范数（的平方）施加一个惩罚，控制系数的整体大小，达到以一定程度的偏误换取更低方差的目的（见 bias-variance tradeoff）。
Formulation 考虑线性模型：$y_i = \bm{x}_i^\prime\bm{\beta} + \varepsilon_i$，$i=1,\dots,N$。其中 $\bm{x}_i$ 和 $\bm{\beta}$ 都是 $p$ 维向量。我们更喜欢表示为矩阵形式：
$$ \bm{Y} = \bm{X}\bm{\beta} + \bm{\mathcal{E}} $$
其中 $\bm{Y} = (y_1,\dots,y_N)^\prime$，$\bm{X} = (\bm{x}_1,\dots,\bm{x}_N)^\prime$，$\bm{\mathcal{E}} = (\varepsilon_1,\dots,\varepsilon_N)^\prime$。假设 $\mathbb{E}(\varepsilon_i|\bm{x}_i) = 0$ 以及 $\mathbb{E}(\varepsilon_i^2|\bm{x}_i) = \sigma^2$，并假设 $\{(\bm{x}_i,y_i)\}_{i=1}^n$ 是独立同分布的。
岭回归估计量 $\hat{\bm{\beta}}$ 来自如下带有惩罚项的最小二乘问题：
$$ \min_{\bm{\beta}}\quad \frac1n(\bm{Y}-\bm{X}\bm{\beta})^\prime(\bm{Y}-\bm{X}\bm{\beta}) + \lambda\bm{\beta}^\prime\bm{\beta} \tag{$*$} $$"><script async src="https://www.googletagmanager.com/gtag/js?id=G-LER46KQN8K"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LER46KQN8K")}</script><script src=/js/toc.js></script><link type=text/css rel=stylesheet href=/vendor/css/bootstrap.min.css><link rel=stylesheet href=/scss/journal.min.ad2da485e6e2508f0b90c8b9bbc39f6fe2a3c27aec57f6da5f61d27ca10988d0.css integrity="sha256-rS2khebiUI8LkMi5u8Ofb+KjwnrsV/baX2HSfKEJiNA=" media=screen><link rel=stylesheet href=/scss/dark-mode.min.f7229ad242ba49e1ea468f47af5b72c60a1e55bb98a14d37522a08158bfbd591.css integrity="sha256-9yKa0kK6SeHqRo9Hr1tyxgoeVbuYoU03UioIFYv71ZE=" media=screen><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lora:500,500italic,700,700italic|Roboto:400,400italic,700,700italic|Montserrat:500,500italic,700,700italic|Fira+Mono:500,700|Noto+Serif+TC:500,700|Noto+Serif+SC:500,700|Material+Icons&display=swap"><script src=https://unpkg.com/@waline/client@v2/dist/waline.js></script><link rel=stylesheet href=https://unpkg.com/@waline/client@v2/dist/waline.css><script>console.log("Hello from 'layouts/partials/extended_head.html'")</script><link rel=stylesheet href=/vendor/css/lxgwwenkaiscreenr.css><script defer src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/js/all.min.js></script><link rel=stylesheet href=/scss/custom/_custom.min.07270570cc4a8650f6229d55a585534fa1253613904e2c8ddf06558255549241.css integrity="sha256-BycFcMxKhlD2Ip1VpYVTT6ElNhOQTiyN3wZVglVUkkE=" media=screen><link rel=stylesheet href=/vendor/css/heti.min.css><script src=/vendor/js/heti.min.js></script><script defer>function domReady(){var e=document.querySelector(".post-subtitle");e&&e.classList.add("heti"),e=document.querySelector("#post-content"),e&&e.classList.add("heti"),document.querySelectorAll(".post-item-summary").forEach(e=>{e.classList.add("heti")})}document.addEventListener("DOMContentLoaded",function(){document.removeEventListener("DOMContentLoaded",arguments.callee,!1),domReady()});const heti=new Heti(".heti");heti.autoSpacing(),window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],macros:{bm:["\\boldsymbol{#1}",1]},processEscapes:!0,processEnvironments:!0},svg:{scale:.962,fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],enableMenu:!1}},function(){var e=document.createElement("script");e.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js",e.async=!0,document.addEventListener("DOMContentLoaded",function(){document.head.appendChild(e)})}()</script><script>document.addEventListener("DOMContentLoaded",function(){var e,t=document.getElementsByClassName("collapsible");for(e=0;e<t.length;e++)t[e].addEventListener("click",function(){this.classList.toggle("active");var e=this.nextElementSibling;e.style.display==="block"?e.style.display="none":e.style.display="block"})})</script></head><body><div id=app><div class=single-column-drawer-container id=drawer v-bind:class="{ 'single-column-drawer-container-active': isDrawerOpen }"><div class=drawer-content><div class=drawer-menu><a class="a-block drawer-menu-item false" href=/about>关于我
</a><a class="a-block drawer-menu-item active" href=/posts>归档
</a><a class="a-block drawer-menu-item false" href=/categories>分类
</a><a class="a-block drawer-menu-item false" href=/tags>标签
</a><a class="a-block drawer-menu-item false" href=/index.xml>订阅</a><div class=toc><div class=toc-content><center>目录</center><ul><ul><li><a href=#%e5%86%85%e5%ae%b9%e6%8f%90%e8%a6%81 class=nav-内容提要>内容提要</a></li><li><a href=#%e5%b2%ad%e5%9b%9e%e5%bd%92%e7%ae%80%e4%bb%8b class=nav-岭回归简介>岭回归简介</a></li><li><a href=#formulation class=nav-formulation>Formulation</a></li><li><a href=#%e6%b8%90%e8%bf%9b%e5%88%86%e5%b8%83 class=nav-渐进分布>渐进分布</a></li><li><a href=#%e5%bb%b6%e4%bc%b8%e6%80%9d%e8%80%83 class=nav-延伸思考>延伸思考</a></li><li><a href=#%e5%8f%82%e8%80%83 class=nav-参考>参考</a></li></ul></div></div></div></div></div><transition name=fade><div id=drawer-mask v-bind:class="{ 'single-column-drawer-mask': mounted }" v-if=isDrawerOpen v-on:click=toggleDrawer></div></transition><nav id=navBar class="navbar sticky-top navbar-light single-column-nav-container"><div id=navBackground class=nav-background></div><div class="container container-narrow nav-content"><button id=nav_dropdown_btn class=nav-dropdown-toggle type=button v-on:click=toggleDrawer>
<i class=material-icons>menu
</i></button>
<a id=navTitle class=navbar-brand href=https://gaxu.xyz/>嘉树的世界
</a><button type=button class=nav-darkmode-toggle id=darkModeToggleButton2>
<i class=material-icons id=darkModeToggleIcon2>dark_mode</i></button></div></nav><div class=single-column-header-container id=pageHead v-bind:style="{ transform: 'translateZ(0px) translateY('+.3*scrollY+'px)', opacity: 1-navOpacity }"><a href=https://gaxu.xyz/><div class=single-column-header-title>嘉树的世界</div><div class=single-column-header-subtitle>靡不有初，鲜克有终</div></a></div><div id=content><div id=streamContainer class=stream-container><div class="post-list-container post-list-container-shadow"><div class=post><div class=post-head-wrapper-text-only><div class=post-title>Ridge Regression<div class=post-meta>嘉树 &nbsp;
<time itemprop=datePublished>2023-03-17 13:37
</time>&emsp;
<i class=material-icons>folder</i>
<a href=/categories/econometrics>Econometrics</a>
&nbsp;
<i class=material-icons>label</i>
<a href=/tags/%E6%B8%90%E8%BF%9B%E5%88%86%E5%B8%83%E7%90%86%E8%AE%BA>渐进分布理论</a></div></div></div><div class=post-body-wrapper><div class=post-body v-pre><div id=post-content><h2 id=内容提要>内容提要&nbsp;<a class=hash-link href=#内容提要><i class="fas fa-link"></i></a></h2><p>一周以前（3月7日）考计量期中，第一题是证明岭回归估计量的渐进分布的，我没有完整地写出来，这里面还是有些技巧，之前我并不熟悉，现在总结在这里。</p><h2 id=岭回归简介>岭回归简介&nbsp;<a class=hash-link href=#岭回归简介><i class="fas fa-link"></i></a></h2><p>Ridge regression，中文译名「岭回归」，是一种利用 <em><a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>正则化方法</a></em> 处理 <em><a href=https://en.wikipedia.org/wiki/Well-posed_problem>不适定问题</a></em> 的技术。我们知道，使用 OLS 时是要求自变量之间不存在完全多重共线性的，即回归元矩阵必须是列满秩的，否则最小二乘问题没有唯一解；但如果某些变量间的相关性很高，甚至接近完全相关，OLS 估计量的方差将会相当大，估计精度大大下降。这个问题在待估参数很多时常常发生。岭回归通过对系数向量的 $L^2$ 范数（的平方）施加一个惩罚，控制系数的整体大小，达到以一定程度的偏误换取更低方差的目的（见 <a href=https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff>bias-variance tradeoff</a>）。</p><h2 id=formulation>Formulation&nbsp;<a class=hash-link href=#formulation><i class="fas fa-link"></i></a></h2><p>考虑线性模型：$y_i = \bm{x}_i^\prime\bm{\beta} + \varepsilon_i$，$i=1,\dots,N$。其中 $\bm{x}_i$ 和 $\bm{\beta}$ 都是 $p$ 维向量。我们更喜欢表示为矩阵形式：</p><p>$$
\bm{Y} = \bm{X}\bm{\beta} + \bm{\mathcal{E}}
$$</p><p>其中 $\bm{Y} = (y_1,\dots,y_N)^\prime$，$\bm{X} = (\bm{x}_1,\dots,\bm{x}_N)^\prime$，$\bm{\mathcal{E}} = (\varepsilon_1,\dots,\varepsilon_N)^\prime$。假设 $\mathbb{E}(\varepsilon_i|\bm{x}_i) = 0$ 以及 $\mathbb{E}(\varepsilon_i^2|\bm{x}_i) = \sigma^2$，并假设 $\{(\bm{x}_i,y_i)\}_{i=1}^n$ 是独立同分布的。</p><p>岭回归估计量 $\hat{\bm{\beta}}$ 来自如下带有惩罚项的最小二乘问题：</p><p>$$
\min_{\bm{\beta}}\quad \frac1n(\bm{Y}-\bm{X}\bm{\beta})^\prime(\bm{Y}-\bm{X}\bm{\beta}) + \lambda\bm{\beta}^\prime\bm{\beta}
\tag{$*$}
$$</p><p>这里 $\lambda$ 是一个参数，它衡量了惩罚的大小。当 $\lambda=0$ 时，该问题就是 OLS；当 $\lambda\to\infty$ 时，$\bm{\beta} = \bm{0}$。事实上，这个问题等价于一个有约束的最小二乘：</p><p>$$
\min_{\bm{\beta}}\quad (\bm{Y}-\bm{X}\bm{\beta})^\prime(\bm{Y}-\bm{X}\bm{\beta}) \qquad \text{s.t.}\quad \bm{\beta}^\prime\bm{\beta} = c^2
$$</p><p>其中 $c$ 为某个非负常数。可见我们是在 $p$ 维 Euclidean 空间中半径为 $c$ 的球面上寻找最小二乘估计量。</p><p>由 $(*)$，一阶条件为 $n^{-1}(-2\bm{X}^\prime\bm{Y}+2\bm{X}^\prime\bm{X}\bm{\beta})+2\lambda\bm{\beta} = \bm{0}$，于是岭回归估计量</p><p>$$
\hat{\bm{\beta}} = \left(\frac1n\bm{X}^\prime\bm{X}+\lambda\bm{I}_p\right)^{-1}\frac1n\bm{X}^\prime\bm{Y}
$$</p><p>其中 $\bm{I}_p$ 是 $p$ 阶单位阵。</p><h2 id=渐进分布>渐进分布&nbsp;<a class=hash-link href=#渐进分布><i class="fas fa-link"></i></a></h2><p>我们把 $\bm{Y} = \bm{X}\bm{\beta} + \bm{\mathcal{E}}$ 带入 $\hat{\bm{\beta}}$ 的表达式，并做适当整理变形，得到</p><p>$$
\hat{\bm{\beta}} - \bm{\beta} = \left(\frac1n\bm{X}^\prime\bm{X}+\lambda\bm{I}_p\right)^{-1}\frac1n\bm{X}^\prime\bm{\mathcal{E}} - \left(\frac1n\bm{X}^\prime\bm{X}+\lambda\bm{I}_p\right)^{-1}\lambda\bm{\beta}
$$</p><p>为简化符号，记 $\bm{B}_i = \bm{x}_i\bm{x}_i^\prime+\lambda\bm{I}_p$，$\bar{\bm{B}}_n = n^{-1}\sum_{i=1}^n\bm{B}_i$，其无条件期望 $\mathbb{E}(\bm{x}_i\bm{x}_i^\prime)+\lambda\bm{I}_p$ 记作 $\bm{B}$；此外，记 $\bm{Q} = \mathbb{E}(\bm{x}_i\bm{x}_i^\prime)$。</p><p>为应用中心极限定理（CLT），需要将上式整理成某个样本均值的形式。事实上，我们应当考虑构造一个更大的随机向量，导出其渐进分布，然后应用 <a href=https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Wold_theorem>Cramér–Wold device</a> 得到估计量的渐进分布。如果我们构造 $(\bm{x}_i^\prime\varepsilon_i,\lambda\bm{\beta}^\prime)^\prime$，则无法直接应用 CLT，因为该向量均值非零（除非 $\bm{\beta} = \bm{0}$）。正确的方式是作如下变形：</p><p>$$
\begin{aligned}
\hat{\bm{\beta}} - \bm{\beta} + \lambda\bm{B}^{-1}\bm{\beta} &= \bar{\bm{B}}_n^{-1}\left[\frac1n\bm{X}^\prime\bm{\mathcal{E}}-(\bm{I}_p-\bar{\bm{B}}_n\bm{B}^{-1})\lambda\bm{\beta}\right] \\
&= \bar{\bm{B}}_n^{-1}\frac1n\sum_{i=1}^n\Bigl[\bm{x}_i\varepsilon_i-(\bm{I}_p-\bm{B}_i\bm{B}^{-1})\lambda\bm{\beta}\Bigr]
\end{aligned}
$$</p><style type=text/css>.notice{--root-color:#444;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#c33;--warning-content:#fee;--info-title:#fb7;--info-content:#fec;--note-title:#6be;--note-content:#e7f2fa;--tip-title:#5a5;--tip-content:#efe}@media(prefers-color-scheme:dark){.notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}}body.night .notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}.notice{padding:0 18px 18px;line-height:24px;margin-bottom:20px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:var(--title-color)}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative}</style><div><svg width="0" height="0" display="none"><symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379.0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628.0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628.0l-22.627 22.627c-6.248 6.248-6.248 16.379.0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"/></symbol><symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405.0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346 7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373.0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884.0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet"><path d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937.0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154.0l239.94 416.028zM288 354c-25.405.0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346 7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373.0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884.0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196.0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627.0 12 5.373 12 12v1e2h12c6.627.0 12 5.373 12 12v24z"/></symbol></svg></div><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>Note</p><p>我们可以将 summand 视为一体，计算其均值和方差，然后直接应用 CLT，这本质上和 Cramér–Wold device 相同。</p></div><p>考虑随机向量 $(\bm{c}_i^\prime,\bm{e}_i^\prime)^\prime$，其中 $\bm{c}_i = \bm{x}_i\varepsilon_i$，$\bm{e}_i = (\bm{I}_p-\bm{B}_i\bm{B}^{-1})\lambda\bm{\beta}$。易见 $\{(\bm{c}_i^\prime,\bm{e}_i^\prime)^\prime\}_{i=1}^n$ 是独立同分布的，均值为 $\bm{0}$，协方差矩阵</p><p>$$
\bm{V} = \begin{pmatrix}
\sigma^2\bm{Q} & \bm{0} \\
\bm{0} & \lambda^2\bm{D}
\end{pmatrix}
$$</p><p>其中 $\bm{D} = \mathbb{E}(\bm{B}_i\bm{B}^{-1}\bm{\beta\beta}^\prime\bm{B}^{-1}\bm{B}_i)-\bm{\beta}\bm{\beta}^\prime$。由 CLT，有</p><p>$$
\frac1{\sqrt{n}}\sum_{i=1}^n\begin{pmatrix}
\bm{c}_i \\ \bm{e}_i
\end{pmatrix} \to_d N(\bm{0},\bm{V})
$$</p><p>于是，利用 Cramér–Wold device，有</p><p>$$
\begin{aligned}
\frac1{\sqrt{n}}\sum_{i=1}^n\Bigl[\bm{x}_i\varepsilon_i-(\bm{I}_p-\bm{B}_i\bm{B}^{-1})\lambda\bm{\beta}\Bigr] &= (1,-1)\frac1{\sqrt{n}}\sum_{i=1}^n\begin{pmatrix}
\bm{c}_i \\ \bm{e}_i
\end{pmatrix} \begin{pmatrix}
1 \\ -1
\end{pmatrix}
\\
&\to_d N(\bm{0}, \sigma^2\bm{Q}+\lambda^2\bm{D})
\end{aligned}
$$</p><p>最后，$\bar{\bm{B}}_n\to_p\bm{B}$，根据 Slutsky 定理，有</p><p>$$
\sqrt{n}(\hat{\bm{\beta}} - \bm{\beta} + \lambda\bm{B}^{-1}\bm{\beta}) \to_d N(\bm{0},\bm{\Omega})
$$</p><p>其中 $\bm{\Omega} = \bm{B}^{-1}(\sigma^2\bm{Q}+\lambda^2\bm{D})\bm{B}^{-1}$。</p><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>Note</p><p>值得一提，Knight and Fu (2000) 也给出了岭回归估计量的渐进分布，但其做法根本上是不同的，且不适用于我们这里的情形。由于他们并未在 $(*)$ 中第一项前添加 $1/n$，所以按照其文中定义，$\lambda_n = n\lambda$ 并非 $O(\sqrt{n})$，故其 Theorem 2 不能适用。</p></div><h2 id=延伸思考>延伸思考&nbsp;<a class=hash-link href=#延伸思考><i class="fas fa-link"></i></a></h2><p>如果我们想构造渐进方差 $\bm{\Omega}$ 的一致估计，则需要构造 $\bm{B},\bm{Q},\bm{D}$ 的一致估计，前两者容易，而 $\bm{D}$ 较为困难。</p><ul><li>记 $\bm{A} = \bm{B}^{-1}\bm{\beta\beta}^\prime\bm{B}^{-1}$。</li><li>$\bm{\beta}$ 的一致估计 $\bar{\bm{\beta}} = (\bm{I}_p - \lambda\bar{\bm{B}}_n^{-1})^{-1}\hat{\bm{\beta}}$。</li><li>$\bm{A}$ 的一致估计 $\bar{\bm{A}} = \bar{\bm{B}}_n^{-1}\bar{\bm{\beta}}\bar{\bm{\beta}}^\prime\bar{\bm{B}}_n^{-1}$。</li></ul><p>乍一看，似乎不能用 $n^{-1}\sum_i\bm{B}_i\bar{\bm{A}}\bm{B}_i$ 作为 $\mathbb{E}(\bm{B}_i\bm{A}\bm{B}_i)$ 的一致估计，因为 $\bar{\bm{A}}$ 本身就是一个估计量，$\bm{B}_i\bar{\bm{A}}\bm{B}_i$ 将不是一列独立随机矩阵，也就无法引用大数定律得到一致性。但实际上是可行的，只需利用 trace 的性质对每个 entry 进行分析：</p><ul><li>$\bm{B}_i\bar{\bm{A}}\bm{B}_i$ 的第 $(k,\ell)$ 元素是 $\bm{b}_{ik}^\prime\bar{\bm{A}}\bm{b}_{i\ell}$，$\bm{b}_{ik}^{\prime}$ 是 $\bm{B}_i$ 的第 $k$ 行，注意 $\bm{B}_i$ 是对称矩阵。</li><li>$\frac1n\sum_i\text{tr}(\bm{b}_{ik}^\prime\bar{\bm{A}}\bm{b}_{i\ell}) = \text{tr}(\frac1n\sum_i\bm{b}_{i\ell}\bm{b}_{ik}^\prime\bar{\bm{A}})$ 依概率收敛到 $\mathbb{E}(\bm{B}_i\bm{A}\bm{B}_i)$ 的第 $(k,\ell)$ 元素，也就是 $\mathbb{E}(\bm{b}_{ik}^\prime\bm{A}\bm{b}_{i\ell}) = \text{tr}(\mathbb{E}[\bm{b}_{i\ell}\bm{b}_{ik}^\prime]\bm{A})$。</li></ul><p>这和 OLS 中异方差时对渐进方差的估计有些相似，比如都是三明治形式。</p><h2 id=参考>参考&nbsp;<a class=hash-link href=#参考><i class="fas fa-link"></i></a></h2><ol><li>&ldquo;<a href=https://en.wikipedia.org/wiki/Ridge_regression>Ridge regression</a>.&rdquo; <em>Wikipedia</em>, accessed March 17, 2023.</li><li>van Wieringen, W. N. &ldquo;Lecture Notes on Ridge Regression.&rdquo; <em>arXiv preprint</em>, submitted in 2015. <a href=https://arxiv.org/pdf/1509.09169.pdf>arXiv:1509.09169</a>.</li><li>Knight, Keith and Wenjiang Fu. 2000. &ldquo;Asymptotics for Lasso-type Estimators.&rdquo; <em>The Annals of Statistics</em>, 28 (5): 1356-1378.</li><li>Hansen, Bruce. 2022. <em>Econometrics</em>. Princeton University Press.</li></ol></div><div><hr width=100% id=EOF style="border-top:.6px solid #777;margin-top:2.5rem"><p style=color:#777>最后修改于 2023-05-01</p></div></div></div><nav class=post-pagination><a class=older-posts href=/posts/bayesian-persuasion/>上回<br><em>Bayesian Persuasion</em>
</a><a class=newer-posts href=/posts/nuclear-norm-min/>下回<br><em>矩阵的秩、核范数与矩阵补全</em></a></nav><div class=post-comment-wrapper><div id=waline></div><script>Waline.init({el:"#waline",dark:"body.night",serverURL:"https://mysitecomments-2yzw3y3tk-cwleo.vercel.app"})</script></div></div></div></div></div><div id=sideContainer class=side-container><a class="a-block nav-head false" href=https://gaxu.xyz/><div class=nav-title>嘉树的世界</div><div class=nav-subtitle>靡不有初，鲜克有终</div></a><div class=nav-link-list><a class="a-block nav-link-item false" href=/about>关于我
</a><a class="a-block nav-link-item active" href=/posts>归档
</a><a class="a-block nav-link-item false" href=/categories>分类
</a><a class="a-block nav-link-item false" href=/tags>标签
</a><a class="a-block nav-link-item false" href=/index.xml>订阅</a></div><div class=nav-footer>由 <a href=https://gohugo.io>Hugo</a> 强力驱动<br>主题改动自 <a href=https://github.com/amazingrise/hugo-theme-diary>Diary</a><br>&copy;
2023–2024 嘉树.
本站遵循 <a href=https://creativecommons.org/licenses/by-nc/4.0/>CC-BY-NC 4.0</a> 协议</div></div><div id=extraContainer class=extra-container><div class=toc-wrapper><div class=toc><div class=toc-content><center>目录</center><ul><ul><li><a href=#%e5%86%85%e5%ae%b9%e6%8f%90%e8%a6%81 class=nav-内容提要>内容提要</a></li><li><a href=#%e5%b2%ad%e5%9b%9e%e5%bd%92%e7%ae%80%e4%bb%8b class=nav-岭回归简介>岭回归简介</a></li><li><a href=#formulation class=nav-formulation>Formulation</a></li><li><a href=#%e6%b8%90%e8%bf%9b%e5%88%86%e5%b8%83 class=nav-渐进分布>渐进分布</a></li><li><a href=#%e5%bb%b6%e4%bc%b8%e6%80%9d%e8%80%83 class=nav-延伸思考>延伸思考</a></li><li><a href=#%e5%8f%82%e8%80%83 class=nav-参考>参考</a></li></ul></div></div></div><div class=pagination><a id=globalBackToTop class="pagination-action animated-visibility" href=#top :class="{ invisible: scrollY == 0 }"><i class="material-icons pagination-action-icon">keyboard_arrow_up
</i></a><a type=button class=pagination-action id=darkModeToggleButton><span class="material-icons pagination-action-icon" id=darkModeToggleIcon>dark_mode</span></a></div></div><div id=single-column-footer>由 <a href=https://gohugo.io>Hugo</a> 强力驱动<br>主题改动自 <a href=https://github.com/amazingrise/hugo-theme-diary>Diary</a><br>&copy;
2023–2024 嘉树.
本站遵循 <a href=https://creativecommons.org/licenses/by-nc/4.0/>CC-BY-NC 4.0</a> 协议</div></div><script src=/js/journal.js></script></body></html>
<!doctype html><html lang=en-US><head><title>LASSO 的收敛</title>
<meta charset=utf-8><meta name=X-UA-Compatible content="IE=edge"><meta name=google-site-verification content><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name=viewport><meta content="telephone=no" name=format-detection><meta name=description content><meta name=renderer content="webkit"><meta name=theme-color content="#ffffff"><meta property="og:url" content="https://gaxu.xyz/posts/lasso/"><meta property="og:site_name" content="嘉树的世界"><meta property="og:title" content="LASSO 的收敛"><meta property="og:description" content="线性模型 通篇考虑线性模型 $$ \bm{Y} = \bm{X}\bm{\beta}^* + \bm{\varepsilon} $$ 其中 $\bm{\varepsilon}\in\text{subG}_n(\sigma^2)$，这表示对任意 $\bm{v}\in\mathbb{R}^n$ 且 $\|\bm{v}\|_2\leq1$，$\bm{v}'\bm{\varepsilon}\in\text{subG}(\sigma^2)$。此外，假设 $\bm{X}$ 是固定的。
LASSO 估计量就是 $$ \hat{\bm{\beta}} := \arg\min_{\bm{\beta}\in\mathbb{R}^p} \frac{1}{2n}\|\bm{Y}-\bm{X}\bm{\beta}\|_2^2 + \lambda_n\|\bm{\beta}\|_1 $$ $\ell_1$ 范数作为惩罚项使得解具有稀疏性 (sparsity)。
预测误差的界 我们考察均方预测误差 (mean squared prediction error, MSPE)： $$ \text{MSPE} = \frac{1}{n}\|\bm{X}(\hat{\bm{\beta}}-\bm{\beta}^*)\|_2^2 $$ 下面我们会给出它的界。
记事件 $A := \{\lambda_n \geq \frac1n\|\bm{X}'\bm{\varepsilon}\|_{\infty}\}$。
Theorem 1."><meta property="og:locale" content="zh"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-04T13:33:52+08:00"><meta property="article:modified_time" content="2024-09-06T20:10:40+08:00"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="高维理论"><meta property="article:tag" content="LASSO"><meta name=twitter:card content="summary"><meta name=twitter:title content="LASSO 的收敛"><meta name=twitter:description content="线性模型 通篇考虑线性模型 $$ \bm{Y} = \bm{X}\bm{\beta}^* + \bm{\varepsilon} $$ 其中 $\bm{\varepsilon}\in\text{subG}_n(\sigma^2)$，这表示对任意 $\bm{v}\in\mathbb{R}^n$ 且 $\|\bm{v}\|_2\leq1$，$\bm{v}'\bm{\varepsilon}\in\text{subG}(\sigma^2)$。此外，假设 $\bm{X}$ 是固定的。
LASSO 估计量就是 $$ \hat{\bm{\beta}} := \arg\min_{\bm{\beta}\in\mathbb{R}^p} \frac{1}{2n}\|\bm{Y}-\bm{X}\bm{\beta}\|_2^2 + \lambda_n\|\bm{\beta}\|_1 $$ $\ell_1$ 范数作为惩罚项使得解具有稀疏性 (sparsity)。
预测误差的界 我们考察均方预测误差 (mean squared prediction error, MSPE)： $$ \text{MSPE} = \frac{1}{n}\|\bm{X}(\hat{\bm{\beta}}-\bm{\beta}^*)\|_2^2 $$ 下面我们会给出它的界。
记事件 $A := \{\lambda_n \geq \frac1n\|\bm{X}'\bm{\varepsilon}\|_{\infty}\}$。
Theorem 1."><script async src="https://www.googletagmanager.com/gtag/js?id=G-LER46KQN8K"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LER46KQN8K")}</script><script src=/js/toc.js></script><link type=text/css rel=stylesheet href=/vendor/css/bootstrap.min.css><link rel=stylesheet href=/scss/journal.min.ad2da485e6e2508f0b90c8b9bbc39f6fe2a3c27aec57f6da5f61d27ca10988d0.css integrity="sha256-rS2khebiUI8LkMi5u8Ofb+KjwnrsV/baX2HSfKEJiNA=" media=screen><link rel=stylesheet href=/scss/dark-mode.min.f7229ad242ba49e1ea468f47af5b72c60a1e55bb98a14d37522a08158bfbd591.css integrity="sha256-9yKa0kK6SeHqRo9Hr1tyxgoeVbuYoU03UioIFYv71ZE=" media=screen><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lora:500,500italic,700,700italic|Roboto:400,400italic,700,700italic|Montserrat:500,500italic,700,700italic|Fira+Mono:500,700|Noto+Serif+TC:500,700|Noto+Serif+SC:500,700|Material+Icons&display=swap"><script src=https://unpkg.com/@waline/client@v2/dist/waline.js></script><link rel=stylesheet href=https://unpkg.com/@waline/client@v2/dist/waline.css><script>console.log("Hello from 'layouts/partials/extended_head.html'")</script><link rel=stylesheet href=/vendor/css/lxgwwenkaiscreenr.css><script defer src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/js/all.min.js></script><link rel=stylesheet href=/scss/custom/_custom.min.1897837e079bbc071513d784df607fc241ee7ec7ff434808792ff70d41b2c157.css integrity="sha256-GJeDfgebvAcVE9eE32B/wkHufsf/Q0gIeS/3DUGywVc=" media=screen><link rel=stylesheet href=/vendor/css/heti.min.css><script src=/vendor/js/heti.min.js></script><script defer>function domReady(){var e=document.querySelector(".post-subtitle");e&&e.classList.add("heti"),e=document.querySelector("#post-content"),e&&e.classList.add("heti"),document.querySelectorAll(".post-item-summary").forEach(e=>{e.classList.add("heti")})}document.addEventListener("DOMContentLoaded",function(){document.removeEventListener("DOMContentLoaded",arguments.callee,!1),domReady()});const heti=new Heti(".heti");heti.autoSpacing(),window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],macros:{bm:["\\boldsymbol{#1}",1]},processEscapes:!0,processEnvironments:!0},svg:{scale:.962,fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],enableMenu:!1}},function(){var e=document.createElement("script");e.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js",e.async=!0,document.addEventListener("DOMContentLoaded",function(){document.head.appendChild(e)})}()</script><script>document.addEventListener("DOMContentLoaded",function(){var e,t=document.getElementsByClassName("collapsible");for(e=0;e<t.length;e++)t[e].addEventListener("click",function(){this.classList.toggle("active");var e=this.nextElementSibling;e.style.display==="block"?e.style.display="none":e.style.display="block"})})</script></head><body><div id=app><div class=single-column-drawer-container id=drawer v-bind:class="{ 'single-column-drawer-container-active': isDrawerOpen }"><div class=drawer-content><div class=drawer-menu><a class="a-block drawer-menu-item false" href=/about>关于我
</a><a class="a-block drawer-menu-item active" href=/posts>归档
</a><a class="a-block drawer-menu-item false" href=/categories>分类
</a><a class="a-block drawer-menu-item false" href=/tags>标签
</a><a class="a-block drawer-menu-item false" href=/index.xml>订阅</a><div class=toc><div class=toc-content><center>目录</center><ul><ul><li><a href=#%e7%ba%bf%e6%80%a7%e6%a8%a1%e5%9e%8b class=nav-线性模型>线性模型</a></li><li><a href=#%e9%a2%84%e6%b5%8b%e8%af%af%e5%b7%ae%e7%9a%84%e7%95%8c class=nav-预测误差的界>预测误差的界</a></li><li><a href=#lambda_n-%e7%9a%84%e9%80%89%e5%8f%96%e5%92%8c-lasso-%e7%9a%84%e6%85%a2%e7%8e%87 class=nav-lambda_n-的选取和-lasso-的慢率>$\lambda_n$ 的选取和 LASSO 的慢率</a></li><li><a href=#lasso-%e7%9a%84%e5%bf%ab%e7%8e%87 class=nav-lasso-的快率>LASSO 的快率</a></li><ul><li><a href=#restricted-eigenvalue class=nav-restricted-eigenvalue>Restricted Eigenvalue</a></li><li><a href=#oracle-inequalities class=nav-oracle-inequalities>Oracle Inequalities</a></li></ul><li><a href=#%e5%8f%82%e8%80%83 class=nav-参考>参考</a></li></ul></div></div></div></div></div><transition name=fade><div id=drawer-mask v-bind:class="{ 'single-column-drawer-mask': mounted }" v-if=isDrawerOpen v-on:click=toggleDrawer></div></transition><nav id=navBar class="navbar sticky-top navbar-light single-column-nav-container"><div id=navBackground class=nav-background></div><div class="container container-narrow nav-content"><button id=nav_dropdown_btn class=nav-dropdown-toggle type=button v-on:click=toggleDrawer>
<i class=material-icons>menu
</i></button>
<a id=navTitle class=navbar-brand href=https://gaxu.xyz/>嘉树的世界
</a><button type=button class=nav-darkmode-toggle id=darkModeToggleButton2>
<i class=material-icons id=darkModeToggleIcon2>dark_mode</i></button></div></nav><div class=single-column-header-container id=pageHead v-bind:style="{ transform: 'translateZ(0px) translateY('+.3*scrollY+'px)', opacity: 1-navOpacity }"><a href=https://gaxu.xyz/><div class=single-column-header-title>嘉树的世界</div><div class=single-column-header-subtitle>靡不有初，鲜克有终</div></a></div><div id=content><div id=streamContainer class=stream-container><div class="post-list-container post-list-container-shadow"><div class=post><div class=post-head-wrapper-text-only><div class=post-title>LASSO 的收敛<div class=post-meta>嘉树 &nbsp;
<time itemprop=datePublished>2024-09-04 13:33
</time>&emsp;
<i class=material-icons>folder</i>
<a href=/categories/econometrics>Econometrics</a>
&nbsp;
<i class=material-icons>label</i>
<a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0>机器学习</a>,
<a href=/tags/%E9%AB%98%E7%BB%B4%E7%90%86%E8%AE%BA>高维理论</a>,
<a href=/tags/lasso>LASSO</a></div></div></div><div class=post-body-wrapper><div class=post-body v-pre><div id=post-content><h2 id=线性模型>线性模型&nbsp;<a class=hash-link href=#线性模型><i class="fas fa-link"></i></a></h2><p>通篇考虑线性模型</p>$$
\bm{Y} = \bm{X}\bm{\beta}^* + \bm{\varepsilon}
$$<p>其中 $\bm{\varepsilon}\in\text{subG}_n(\sigma^2)$，这表示对任意 $\bm{v}\in\mathbb{R}^n$ 且 $\|\bm{v}\|_2\leq1$，$\bm{v}'\bm{\varepsilon}\in\text{subG}(\sigma^2)$。此外，假设 $\bm{X}$ 是固定的。</p><p>LASSO 估计量就是</p>$$
\hat{\bm{\beta}} := \arg\min_{\bm{\beta}\in\mathbb{R}^p} \frac{1}{2n}\|\bm{Y}-\bm{X}\bm{\beta}\|_2^2 + \lambda_n\|\bm{\beta}\|_1
$$<p>$\ell_1$ 范数作为惩罚项使得解具有稀疏性 (sparsity)。</p><h2 id=预测误差的界>预测误差的界&nbsp;<a class=hash-link href=#预测误差的界><i class="fas fa-link"></i></a></h2><p>我们考察均方预测误差 (mean squared prediction error, MSPE)：</p>$$
\text{MSPE} = \frac{1}{n}\|\bm{X}(\hat{\bm{\beta}}-\bm{\beta}^*)\|_2^2
$$<p>下面我们会给出它的界。</p><p>记事件 $A := \{\lambda_n \geq \frac1n\|\bm{X}'\bm{\varepsilon}\|_{\infty}\}$。</p><div class=thm><p id=thm:consistency class="thm-title thm-theorem">Theorem 1.</p><div class="thm-inner thm-theorem"><p>若事件 $A$ 成立，则</p>$$
\frac{1}{n}\|\bm{X}(\hat{\bm{\beta}}-\bm{\beta}^*)\|_2^2 \leq 4\|\bm{\beta}^*\|_1\lambda_n
$$</div></div><div class=thm><button type=button class=collapsible><div id=thm-proof-1 class=thm-title-proof>Proof.</div></button><div class=proof-content><div class=thm-inner-proof><p>首先，我们证明如下基本不等式：</p>$$
\frac{1}{2n}\|\bm{X}(\hat{\bm{\beta}}-\bm{\beta}^*)\|_2^2 \leq \frac{1}{n}\bm{\varepsilon}'\bm{X}(\hat{\bm{\beta}}-\bm{\beta}^*) + \lambda_n(\|\bm{\beta}^*\|_1-\|\hat{\bm{\beta}}\|_1)
$$<p>此不等式事实上直接来自于</p>$$
\frac{1}{2n}\|\bm{Y}-\bm{X}\hat{\bm{\beta}}\|_2^2 + \lambda_n\|\hat{\bm{\beta}}\|_1 \leq \frac{1}{2n}\|\bm{Y}-\bm{X}\bm{\beta}^*\|_2^2 + \lambda_n\|\bm{\beta}^*\|_1
$$<p>只需将 $\bm{Y} = \bm{X}\bm{\beta}^* + \bm{\varepsilon}$ 代入即可得到。</p><p>以下给出 $\frac{1}{2n}\|\bm{X}(\hat{\bm{\beta}}-\bm{\beta}^*)\|_2^2$ 的界：</p>$$
\begin{align*}
\frac{1}{2n}\|\bm{X}(\hat{\bm{\beta}}-\bm{\beta}^*)\|_2^2 &\leq \frac{1}{n}\bm{\varepsilon}'\bm{X}(\hat{\bm{\beta}}-\bm{\beta}^*) + \lambda_n(\|\bm{\beta}^*\|_1-\|\hat{\bm{\beta}}\|_1) \\
&\leq \frac{1}{n}\|\bm{X}'\bm{\varepsilon}\|_\infty\|\hat{\bm{\beta}}-\bm{\beta}^*\|_1 + \lambda_n(\|\bm{\beta}^*\|_1-\|\hat{\bm{\beta}}\|_1) \\
&\leq \frac{1}{n}\|\bm{X}'\bm{\varepsilon}\|_\infty\|(\|\hat{\bm{\beta}}\|_1+\|\bm{\beta}\|_1) + \lambda_n(\|\bm{\beta}^*\|_1-\|\hat{\bm{\beta}}\|_1) \\
&= \|\hat{\bm{\beta}}\|_1\biggl(\frac{1}{n}\|\bm{X}'\bm{\varepsilon}\|_\infty-\lambda_n\biggr) + \|\bm{\beta}^*\|_1\biggl(\frac{1}{n}\|\bm{X}'\bm{\varepsilon}\|_\infty+\lambda_n\biggr) \\
&\leq 2\lambda_n\|\bm{\beta}^*\|_1
\end{align*}
$$<p>第二行使用了 Hölder 不等式，第三行使用了三角不等式，最后一行是因为事件 $A$ 成立。</p></div><div class=thm-end-proof><i class="fas fa-square"></i> <a href=#thm-proof-1><i class="fas fa-arrow-circle-up"></i></a></div></div></div><p>关于此定理有几点需要注意：</p><ol><li>不等式所给出的界取决于调谐参数 $\lambda_n$，而它又取决于样本量 $n$。</li><li>此不等式在事件 $A$ 上成立，因此如果事件 $A$ 有很大概率发生，则此不等式也有很大概率发生（不低于事件 $A$ 发生的概率）。</li></ol><h2 id=lambda_n-的选取和-lasso-的慢率>$\lambda_n$ 的选取和 LASSO 的慢率&nbsp;<a class=hash-link href=#lambda_n-的选取和-lasso-的慢率><i class="fas fa-link"></i></a></h2><p>我们当然希望 $\lambda_n$ 越小越好，我们特别希望 $\lambda_n\|\bm{\beta}^*\|=o(1)$，因为这样预测误差就会收敛。但是我们又要保证事件 $A$ 发生的概率，所以 $\lambda_n$ 又不能过小，或者说我们要限制 $\frac1n\|\bm{X}'\bm{\varepsilon}\|_{\infty}$。为此，我们对 $\bm{X}$ 施加一个额外的假设。</p><div class=thm><p id=assump:max class="thm-title thm-assumption">Assumption 2.</p><div class="thm-inner thm-assumption">设 $\max_j\|\bm{X}_j\|\leq\sqrt{Cn}$ 对某个 $C>0$ 成立，这里 $\bm{X}_j$ 是 $\bm{X}$ 的第 $j$ 列。</div></div><p>在这个假设下，对任意 $t>0$ 我们有</p>$$
\begin{align*}
\mathbb{P}\biggl\{\frac{1}{n}\|\bm{X}'\bm{\varepsilon}\|_\infty\geq t\biggr\} &= \mathbb{P}\biggl\{\frac{1}{n}\max_j|\bm{X}_j'\bm{\varepsilon}|\geq t\biggr\} \\
&\leq \sum_j \mathbb{P}\biggl\{\frac{1}{n}|\bm{X}_j'\bm{\varepsilon}|\geq t\biggr\} \\
&\leq \sum_j \mathbb{P}\biggl\{\frac{|\bm{X}_j'\bm{\varepsilon}|}{n\|\bm{X}_j\|_2}\geq\frac{t}{\|\bm{X}_j\|_2}\biggr\} \\
&\leq 2p\exp\biggl(-\frac{t^2n}{2\sigma^2C}\biggr)
\end{align*}
$$<p>最后一行利用了 sub-Gaussian 的性质：如果 $X\in\text{subG}(\sigma^2)$，则对任意 $t>0$ 有</p>$$
\mathbb{P}\{X\geq t\} \leq \exp\biggl(-\frac{t^2}{2\sigma^2}\biggr)
$$<p>给定一个 $\delta>0$，我们选择</p>$$
t = \sqrt{\frac{2\sigma^2C}{n}[\log(1/\delta)+\log(2p)]}
$$<p>那么以上概率就会</p>$$
\mathbb{P}\biggl\{\frac{1}{n}\|\bm{X}'\bm{\varepsilon}\|_\infty\geq t\biggr\} \leq 2p\exp\biggl(-\frac{t^2n}{2\sigma^2C}\biggr) \leq \delta
$$<p>也就是说，我们取 $\lambda_n$ 为上述 $t$ 值，那么事件 $A$ 发生的概率就至少是 $1-\delta$。我们有以下推论。</p><div class=thm><p id=coro:slowrate class="thm-title thm-corollary">Corollary 3 (Slow rate for the LASSO).</p><div class="thm-inner thm-corollary"><p>至少以概率 $1-\delta$，我们有</p>$$
\frac{1}{n}\|\bm{X}(\hat{\bm{\beta}}-\bm{\beta}^*)\|_2^2 \leq 4\|\bm{\beta}^*\|_1\sigma\sqrt{\frac{2C}{n}[\log(1/\delta)+\log(2p)]}
$$</div></div><p>可见，当我们取 $\delta=1/n$，并假设 $\|\bm{\beta}^*\|=o(\sqrt{n/\log(p\vee n)})$，我们就得到了随 $n\to\infty$ 时预测误差的收敛。</p><style type=text/css>.notice{--root-color:#444;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#c33;--warning-content:#fee;--info-title:#fb7;--info-content:#fec;--note-title:#6be;--note-content:#e7f2fa;--tip-title:#5a5;--tip-content:#efe}@media(prefers-color-scheme:dark){.notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}}body.night .notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}.notice{padding:0 18px 18px;line-height:24px;margin-bottom:20px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:var(--title-color)}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative}</style><div><svg width="0" height="0" display="none"><symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379.0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628.0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628.0l-22.627 22.627c-6.248 6.248-6.248 16.379.0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"/></symbol><symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405.0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346 7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373.0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884.0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet"><path d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937.0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154.0l239.94 416.028zM288 354c-25.405.0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346 7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373.0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884.0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196.0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627.0 12 5.373 12 12v1e2h12c6.627.0 12 5.373 12 12v24z"/></symbol></svg></div><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>Note</p><p>为什么叫 slow rate？在最优子集选择 (best subset selection) 中，我们可以推出 (reference?)</p>$$
\frac{1}{n}\|\bm{X}(\hat{\bm{\beta}}-\bm{\beta}^*)\|_2^2 \lesssim \|\bm{\beta}\|_0\frac{\log n+\log p}{n}
$$<p>而在上述推论中，如果我们选择 $\delta=1/n$，则会得到</p>$$
\frac{1}{n}\|\bm{X}(\hat{\bm{\beta}}-\bm{\beta}^*)\|_2^2 \lesssim \|\bm{\beta}^*\|_1\sqrt{\frac{\log n+\log p}{n}}
$$<p>这多了一个根号，因此比最优子集选择的收敛率要慢。</p></div><h2 id=lasso-的快率>LASSO 的快率&nbsp;<a class=hash-link href=#lasso-的快率><i class="fas fa-link"></i></a></h2><p>我们是否能改进前述收敛速率呢？另外，有没有可能得到 $\hat{\bm{\beta}}$ 本身的一致性呢？答案是可以的。</p><h3 id=restricted-eigenvalue>Restricted Eigenvalue&nbsp;<a class=hash-link href=#restricted-eigenvalue><i class="fas fa-link"></i></a></h3><p>注意到如果 $\mu_{\min}(\bm{X}'\bm{X})\geq\gamma_n>0$，则有</p>$$
\|\hat{\bm{\beta}}-\bm{\beta}^*\|_2 \leq \frac{1}{\gamma_n}\|\bm{X}(\hat{\bm{\beta}}-\bm{\beta}^*)\|_2^2
$$<p>而当 $p>n$ 时，$\mu_{\min}(\bm{X}'\bm{X})=0$，此时我们无法得到 $\|\hat{\bm{\beta}}-\bm{\beta}^*\|_2$ 的收敛。我们需要对 $\bm{X}$ 施加额外的假设，有很多种方式，但基本思想就一个：让 $\gamma_n$ 和零保持距离。</p><p>以下介绍一种广泛使用的假设：受限特征值条件 (restricted eigenvalue condition, RE)。</p><div class=thm><p id=def:re class="thm-title thm-definition">Definition 4 (Restricted Eigenvalue).</p><div class="thm-inner thm-definition"><p>给定 $\alpha\geq1$ 和 $S\subseteq\{1,\dots,p\}$ 且 $S\neq\emptyset$，令</p>$$
\mathcal{C}_\alpha(S) = \{\bm{\Delta}\in\mathbb{R}^p\colon\|\bm{\Delta}_{S^\complement}\|_1\leq\alpha\|\bm{\Delta}_S\|_1\}
$$<p>其中 $S^\complement = \{1,\dots,p\}\backslash S$，$\bm{\Delta}_{S}=(\Delta_j)_{j\in S}$。</p><p>如果存在某个 $\alpha\geq1$ 和 $\kappa>0$，对任意的 $\bm{\Delta}\in\mathcal{C}_\alpha(S)$ 都有</p>$$
\frac{1}{n}\|\bm{X}\bm{\Delta}\|_2^2 \geq \kappa\|\bm{\Delta}\|_2^2
$$<p>则称 $n\times p$ 的矩阵 $\bm{X}$ 满足 $S$ 上的 $\text{RE}(\alpha,\kappa)$ 条件。</p></div></div><p>下面我们花一些篇幅来从直觉上理解这一条件。</p><p>取 $\bm{\Delta}=\hat{\bm{\beta}}-\bm{\beta}^*$。前面我们已经证明了 $\frac1n\|\bm{X}\bm{\Delta}\|_2^2$ 能够以大概率取很小的值，但这不意味着 $\|\bm{\Delta}\|_2^2$ 也可以很小，因为函数 $\bm{\Delta}\mapsto\frac1n\|\bm{X}\bm{\Delta}\|_2^2$ 在 $\hat{\bm{\beta}}-\bm{\beta}^*$ 附近可能很平坦，也就是说，在较宽的一个范围内我们都能得到较小的损失函数值。</p><p>如果 $\mu_{\min}(\bm{X}'\bm{X})/n$ 和零保持一定距离，譬如</p>$$
\kappa\|\bm{\Delta}\|_2^2 \leq \frac{1}{n}\|\bm{X}\bm{\Delta}\|_2^2
$$<p>这将意味着 $\bm{\Delta}\mapsto\frac1n\|\bm{X}\bm{\Delta}\|_2^2$ 有足够的曲率（不是平坦的曲线）。我们只需要这个条件对某些 $\bm{\Delta}$ 成立，即 RE 所表述的那样。这些 $\bm{\Delta}$ 被集合 $\mathcal{C}_\alpha(S)$ 所刻画，它是这样一个集合：指标集 $S$ 所对应的那些系数要充分大于非 $S$ 对应的系数。这直觉上规定了 $S$ 和 $S^\complement$ 对应的系数值能被区分开的一类系数向量。</p><p>令 $S = \{j^*\colon\beta_j^*\neq0\}$ 是非零系数的指标集，称之为 <em>active set</em>，$s=|S|$ 是非零系数的个数，称之为<em>稀疏性</em>。在 RE 条件下，有如下定理。</p><div class=thm><p id=thm:fast class="thm-title thm-theorem">Theorem 5 (Fast rate of the LASSO).</p><div class="thm-inner thm-theorem"><p>设存在某个 $\kappa>0$ 使得 $\bm{X}$ 满足 $S$ 上的 $\text{RE}(3,\kappa)$ 条件。若</p>$$
\lambda_n \geq \frac{2}{n}\|\bm{X}'\bm{\varepsilon}\|_\infty
$$<p>则 LASSO 估计量 $\hat{\beta}$ 满足</p>$$
\begin{align*}
\frac{1}{n}\|\bm{X}(\hat{\bm{\beta}}-\bm{\beta}^*)\|_2^2 &\leq \frac{9s\lambda_n^2}{\kappa} \\
\|\hat{\bm{\beta}}-\bm{\beta}^*\|_2 &\leq \frac{3\sqrt{s}\lambda_n}{\kappa}
\end{align*}
$$</div></div><div class=thm><button type=button class=collapsible><div id=thm-proof-2 class=thm-title-proof>Proof.</div></button><div class=proof-content><div class=thm-inner-proof><p>首先证明，当 $\lambda_n$ 满足上述定理的条件时，有 $\hat{\bm{\Delta}}=\hat{\bm{\beta}}-\bm{\beta}^*\in\mathcal{C}_3(S)$。</p><p>因 $\hat{\bm{\beta}}$ 是最小化问题的解，有</p>$$
\frac{1}{2n}\|\bm{Y}-\bm{X}\hat{\bm{\beta}}\|_2^2 + \lambda_n\|\hat{\bm{\beta}}\|_1 \leq \frac{1}{2n}\|\bm{Y}-\bm{X}\bm{\beta}^*\|_2^2 + \lambda_n\|\bm{\beta}^*\|_1
$$<p>代入 $\bm{Y} = \bm{X}\bm{\beta}^* + \bm{\varepsilon}$，整理得到</p>$$
0 \leq \frac{1}{2n}\|\bm{X}\hat{\bm{\Delta}}\|_2^2 \leq \frac1n\bm{\varepsilon}'\bm{X}\hat{\bm{\Delta}} + \lambda_n(\|\bm{\beta}^*\|_1-\|\hat{\bm{\beta}}\|_1)
$$<p>根据 $S$ 的定义，$\|\bm{\beta}^*\|_1 = \|\bm{\beta}_S^*\|_1$。另外，可将 $\|\hat{\bm{\beta}}\|_1$ 写为</p>$$
\|\hat{\bm{\beta}}\|_1 = \|\hat{\bm{\beta}}_S\|_1 + \|\hat{\bm{\beta}}_{S^\complement}\|_1 = \|\bm{\beta}_S^*+\hat{\bm{\Delta}}_S\|_1 + \|\hat{\bm{\Delta}}_{S^\complement}\|_1
$$<p>带入到前面的不等式得到</p>$$
\begin{align*}
0 \leq \frac{1}{2n}\|\bm{X}\hat{\bm{\Delta}}\|_2^2 &\leq \frac1n\bm{\varepsilon}'\bm{X}\hat{\bm{\Delta}} + \lambda_n(\|\bm{\beta}_S^*\|_1-\|\bm{\beta}_S^*+\hat{\bm{\Delta}}_S\|_1 - \|\hat{\bm{\beta}}_{S^\complement}\|_1) \\
&\leq \frac{1}{n}\|\bm{X}'\bm{\varepsilon}\|_\infty\|\hat{\bm{\Delta}}\|_1 + \lambda_n(\|\bm{\beta}_S^*\|_1-\|\bm{\beta}_S^*+\hat{\bm{\Delta}}_S\|_1 - \|\hat{\bm{\Delta}}_{S^\complement}\|_1) \\
&\leq \frac{1}{n}\|\bm{X}'\bm{\varepsilon}\|_\infty\|\hat{\bm{\Delta}}\|_1 + \lambda_n(\|\hat{\bm{\Delta}}_S\|_1 - \|\hat{\bm{\Delta}}_{S^\complement}\|_1) \\
&\leq \frac12\lambda_n\|\hat{\bm{\Delta}}\|_1 + \lambda_n(\|\hat{\bm{\Delta}}_S\|_1 - \|\hat{\bm{\Delta}}_{S^\complement}\|_1) \\
&= \frac12\lambda_n\|\hat{\bm{\Delta}}_S\|_1 + \frac12\lambda_n\|\hat{\bm{\Delta}}_{S^\complement}\|_1 + \lambda_n(\|\hat{\bm{\Delta}}_S\|_1 - \|\hat{\bm{\Delta}}_{S^\complement}\|_1) \\
&= \frac{3}{2}\lambda_n\|\hat{\bm{\Delta}}_S\|_1 - \frac12\lambda_n\|\hat{\bm{\Delta}}_{S^\complement}\|_1
\end{align*}
$$<p>第二行使用了 Hölder 不等式，第三行使用了三角不等式，第四行使用了条件 $\lambda_n \geq \frac{2}{n}\|\bm{X}'\bm{\varepsilon}\|_\infty$。这就导致了 $\|\hat{\bm{\Delta}}_{S^\complement}\|_1 \leq 3\|\hat{\bm{\Delta}}_S\|_1$，也就证明了 $\hat{\bm{\Delta}}\in\mathcal{C}_3(S)$。</p><p>第二步，我们将使用 RE 条件。注意到</p>$$
\lambda_n(3\|\hat{\bm{\Delta}}_S\|_1-\|\hat{\bm{\Delta}}_{S^\complement}\|_1) \leq 3\lambda_n\|\hat{\bm{\Delta}}_S\|_1 \leq 3\lambda_n\sqrt{s}\|\hat{\bm{\Delta}}_S\|_2 \leq 3\lambda_n\sqrt{s}\|\hat{\bm{\Delta}}\|_2
$$<p>这里使用了 $\ell_1$ 和 $\ell_2$ 范数之间的关系：对 $\bm{x}\in\mathbb{R}^d$，有 $\|\bm{x}\|_2\leq\|\bm{x}\|_1\leq\sqrt{d}\|\bm{x}\|_2$，这可通过数形结合来理解。从而</p>$$
\begin{align*}
\frac{1}{n}\|\bm{X}\hat{\bm{\Delta}}\|_2^2 &\leq \lambda_n(3\|\hat{\bm{\Delta}}_S\|_1-\|\hat{\bm{\Delta}}_{S^\complement}\|_1) \\
&\leq 3\lambda_n\sqrt{s}\|\hat{\bm{\Delta}}\|_2 \\
&\leq 3\lambda_n\sqrt{s}\frac{\|\bm{X}\hat{\bm{\Delta}}\|_2}{\sqrt{n\kappa}}
\end{align*}
$$<p>最后一行用了 RE 条件。整理后就得到</p>$$
\frac{1}{n}\|\bm{X}\hat{\bm{\Delta}}\|_2^2 \leq \frac{9s\lambda_n^2}{\kappa}
$$<p>使用此结论和 RE 就有第二个收敛</p>$$
\begin{align*}
\|\hat{\bm{\Delta}}\|_2 \leq \frac{\|\bm{X}\hat{\bm{\Delta}}\|_2}{\sqrt{n\kappa}} \leq \sqrt{\frac{9s\lambda_n^2}{\kappa^2}} = \frac{3\sqrt{s}\lambda_n}{\kappa}
\end{align*}
$$</div><div class=thm-end-proof><i class="fas fa-square"></i> <a href=#thm-proof-2><i class="fas fa-arrow-circle-up"></i></a></div></div></div><p>很显然，相比于 <a href=/posts/lasso/#thm:consistency>定理 1</a>，我们把不等式右边的界的收敛速度提升了（$\lambda_n$ 提升到 $\lambda_n^2$），此外还得到了 $\|\hat{\bm{\beta}}-\bm{\beta}^*\|_2$ 的收敛，所以称之为 LASSO 的快率。</p><p>我们也可以像 <a href=/posts/lasso/#coro:slowrate>推论 3</a> 那样用概率的形式表述，这里略去。</p><h3 id=oracle-inequalities>Oracle Inequalities&nbsp;<a class=hash-link href=#oracle-inequalities><i class="fas fa-link"></i></a></h3><div class="notice info"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#info-notice"/></svg></span>Info</p><p>在英文里面，“oracle” 一词的含义是神的使者，向（愚昧）的人类传达神的旨意和预言，引申为提供智慧、知识或真理的源头。而在统计学中，这一词被借用来指代理论上所能达到的最佳表现，如同有一个神使向我们传达了真理。Oracle inequalities，「神谕不等式」，就描述了一个估计量的性能如何接近理论上最佳的性能。</p></div><p>我们知道，当 $p\leq n$ 且误差项 $\bm{\varepsilon}\sim N(0,\sigma^2\bm{I}_n)$ 时，OLS 是最优线性无偏估计量，它可以作为一个最优的标杆。由于 $\|\bm{X}(\hat{\bm{\beta}}^{\text{OLS}}-\bm{\beta}^*)\|_2^2/$ 服从 $\chi^2(p)$ 分布，因此期望预测误差</p>$$
\mathbb{E}\biggl(\frac{1}{n}\|\bm{X}(\hat{\bm{\beta}}^{\text{OLS}}-\bm{\beta}^*)\|_2^2\biggr) = \frac{\sigma^2p}{n}
$$<p>这表明，每个系数的估计精度是 $\sigma^2/n$，所以总体的估计精度就是 $(\sigma^2/n)\cdot p$。</p><p>当 $p>n$ 时，OLS 不再适用，但假设我们知道了 $\bm{\beta}^*$ 的稀疏度 $s$，按照上面的公式，能达到的最佳估计精度就应该是 $(\sigma^2/n)\cdot s$。那么，LASSO 是否达到了或接近了这一理想呢？在 <a href=/posts/lasso/#thm:fast>前一节的定理</a> 中，LASSO 的预测误差的界是</p>$$
\frac{1}{n}\|\bm{X}(\hat{\bm{\beta}}-\bm{\beta}^*)\|_2^2 \leq \frac{9s\lambda_n^2}{\kappa}
$$<p>如果我们采取前文所选取的 $\lambda_n$，那么我们就有</p>$$
\frac{1}{n}\|\bm{X}(\hat{\bm{\beta}}-\bm{\beta}^*)\|_2^2 \lesssim \sigma^2\frac{s\log p}{n}
$$<p>相比于理想中的最佳估计精度，我们多了一个因子 $\log p$，这可以被看作不知道哪些系数非零所付出的成本。这就是 LASSO 的神谕不等式。</p><h2 id=参考>参考&nbsp;<a class=hash-link href=#参考><i class="fas fa-link"></i></a></h2><p>Alessandro Rinaldo&rsquo;s Lecture Notes (2018): <a href=https://www.stat.cmu.edu/~arinaldo/Teaching/36710/F18/Scribed_Lectures/Oct17.pdf>Oct 17</a>, <a href=https://www.stat.cmu.edu/~arinaldo/Teaching/36710/F18/Scribed_Lectures/Oct22.pdf>Oct 22</a>.</p><p><a href=https://www.stt.msu.edu/users/wangho16/lasso.pdf>Honglang Wang&rsquo;s Tutorial on Lasso</a>.</p><p>Bühlmann, Peter, and Sara van de Geer (2011). <em>Statistics for High-Dimensional Data: Methods, Theory and Applications.</em> Springer Science & Business Media.</p><p>Rigollet, Philippe, and Jan-Christian Hütter (2023). &ldquo;High-Dimensional Statistics.&rdquo; <em>arXiv preprint</em>. <a href=https://arxiv.org/pdf/2310.19244.pdf>arXiv:2310.19244</a>.</p></div><div><hr width=100% id=EOF style="border-top:.6px solid #777;margin-top:2.5rem"><p style=color:#777>最后修改于 2024-09-06</p></div></div></div><nav class=post-pagination><a class=older-posts href=/posts/multiple-gh-accounts/>上回<br><em>在 Mac 上管理多个 GitHub 账号</em>
</a><a class=newer-posts>下回<br>已经到头啦。</a></nav><div class=post-comment-wrapper><div id=waline></div><script>Waline.init({el:"#waline",dark:"body.night",serverURL:"https://mysitecomments-2yzw3y3tk-cwleo.vercel.app"})</script></div></div></div></div></div><div id=sideContainer class=side-container><a class="a-block nav-head false" href=https://gaxu.xyz/><div class=nav-title>嘉树的世界</div><div class=nav-subtitle>靡不有初，鲜克有终</div></a><div class=nav-link-list><a class="a-block nav-link-item false" href=/about>关于我
</a><a class="a-block nav-link-item active" href=/posts>归档
</a><a class="a-block nav-link-item false" href=/categories>分类
</a><a class="a-block nav-link-item false" href=/tags>标签
</a><a class="a-block nav-link-item false" href=/index.xml>订阅</a></div><div class=nav-footer>由 <a href=https://gohugo.io>Hugo</a> 强力驱动<br>主题改动自 <a href=https://github.com/amazingrise/hugo-theme-diary>Diary</a><br>&copy;
2023–2024 嘉树.
本站遵循 <a href=https://creativecommons.org/licenses/by-nc/4.0/>CC-BY-NC 4.0</a> 协议</div></div><div id=extraContainer class=extra-container><div class=toc-wrapper><div class=toc><div class=toc-content><center>目录</center><ul><ul><li><a href=#%e7%ba%bf%e6%80%a7%e6%a8%a1%e5%9e%8b class=nav-线性模型>线性模型</a></li><li><a href=#%e9%a2%84%e6%b5%8b%e8%af%af%e5%b7%ae%e7%9a%84%e7%95%8c class=nav-预测误差的界>预测误差的界</a></li><li><a href=#lambda_n-%e7%9a%84%e9%80%89%e5%8f%96%e5%92%8c-lasso-%e7%9a%84%e6%85%a2%e7%8e%87 class=nav-lambda_n-的选取和-lasso-的慢率>$\lambda_n$ 的选取和 LASSO 的慢率</a></li><li><a href=#lasso-%e7%9a%84%e5%bf%ab%e7%8e%87 class=nav-lasso-的快率>LASSO 的快率</a></li><ul><li><a href=#restricted-eigenvalue class=nav-restricted-eigenvalue>Restricted Eigenvalue</a></li><li><a href=#oracle-inequalities class=nav-oracle-inequalities>Oracle Inequalities</a></li></ul><li><a href=#%e5%8f%82%e8%80%83 class=nav-参考>参考</a></li></ul></div></div></div><div class=pagination><a id=globalBackToTop class="pagination-action animated-visibility" href=#top :class="{ invisible: scrollY == 0 }"><i class="material-icons pagination-action-icon">keyboard_arrow_up
</i></a><a type=button class=pagination-action id=darkModeToggleButton><span class="material-icons pagination-action-icon" id=darkModeToggleIcon>dark_mode</span></a></div></div><div id=single-column-footer>由 <a href=https://gohugo.io>Hugo</a> 强力驱动<br>主题改动自 <a href=https://github.com/amazingrise/hugo-theme-diary>Diary</a><br>&copy;
2023–2024 嘉树.
本站遵循 <a href=https://creativecommons.org/licenses/by-nc/4.0/>CC-BY-NC 4.0</a> 协议</div></div><script src=/js/journal.js></script></body></html>
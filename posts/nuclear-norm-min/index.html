<!doctype html><html lang=en-US><head><title>矩阵的秩、核范数与矩阵补全</title>
<meta charset=utf-8><meta name=X-UA-Compatible content="IE=edge"><meta name=google-site-verification content><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name=viewport><meta content="telephone=no" name=format-detection><meta name=description content="矩阵核范数之于秩，一如向量 $\ell_1$ 范数之于 $\ell_0$ 范数。"><meta name=renderer content="webkit"><meta name=theme-color content="#ffffff"><meta property="og:url" content="https://gaxu.xyz/posts/nuclear-norm-min/"><meta property="og:site_name" content="嘉树的世界"><meta property="og:title" content="矩阵的秩、核范数与矩阵补全"><meta property="og:description" content="矩阵核范数之于秩，一如向量 $\ell_1$ 范数之于 $\ell_0$ 范数。"><meta property="og:locale" content="zh"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-04-08T16:06:52+08:00"><meta property="article:modified_time" content="2023-04-11T00:02:42+08:00"><meta property="article:tag" content="矩阵范数"><meta name=twitter:card content="summary"><meta name=twitter:title content="矩阵的秩、核范数与矩阵补全"><meta name=twitter:description content="矩阵核范数之于秩，一如向量 $\ell_1$ 范数之于 $\ell_0$ 范数。"><script async src="https://www.googletagmanager.com/gtag/js?id=G-LER46KQN8K"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LER46KQN8K")}</script><script src=/js/toc.js></script><link type=text/css rel=stylesheet href=/vendor/css/bootstrap.min.css><link rel=stylesheet href=/scss/journal.min.ad2da485e6e2508f0b90c8b9bbc39f6fe2a3c27aec57f6da5f61d27ca10988d0.css integrity="sha256-rS2khebiUI8LkMi5u8Ofb+KjwnrsV/baX2HSfKEJiNA=" media=screen><link rel=stylesheet href=/scss/dark-mode.min.f7229ad242ba49e1ea468f47af5b72c60a1e55bb98a14d37522a08158bfbd591.css integrity="sha256-9yKa0kK6SeHqRo9Hr1tyxgoeVbuYoU03UioIFYv71ZE=" media=screen><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lora:500,500italic,700,700italic|Roboto:400,400italic,700,700italic|Montserrat:500,500italic,700,700italic|Fira+Mono:500,700|Noto+Serif+TC:500,700|Noto+Serif+SC:500,700|Material+Icons&display=swap"><script src=https://unpkg.com/@waline/client@v2/dist/waline.js></script><link rel=stylesheet href=https://unpkg.com/@waline/client@v2/dist/waline.css><script>console.log("Hello from 'layouts/partials/extended_head.html'")</script><link rel=stylesheet href=/vendor/css/lxgwwenkaiscreenr.css><script defer src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/js/all.min.js></script><link rel=stylesheet href=/scss/custom/_custom.min.07270570cc4a8650f6229d55a585534fa1253613904e2c8ddf06558255549241.css integrity="sha256-BycFcMxKhlD2Ip1VpYVTT6ElNhOQTiyN3wZVglVUkkE=" media=screen><link rel=stylesheet href=/vendor/css/heti.min.css><script src=/vendor/js/heti.min.js></script><script defer>function domReady(){var e=document.querySelector(".post-subtitle");e&&e.classList.add("heti"),e=document.querySelector("#post-content"),e&&e.classList.add("heti"),document.querySelectorAll(".post-item-summary").forEach(e=>{e.classList.add("heti")})}document.addEventListener("DOMContentLoaded",function(){document.removeEventListener("DOMContentLoaded",arguments.callee,!1),domReady()});const heti=new Heti(".heti");heti.autoSpacing(),window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],macros:{bm:["\\boldsymbol{#1}",1]},processEscapes:!0,processEnvironments:!0},svg:{scale:.962,fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],enableMenu:!1}},function(){var e=document.createElement("script");e.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js",e.async=!0,document.addEventListener("DOMContentLoaded",function(){document.head.appendChild(e)})}()</script><script>document.addEventListener("DOMContentLoaded",function(){var e,t=document.getElementsByClassName("collapsible");for(e=0;e<t.length;e++)t[e].addEventListener("click",function(){this.classList.toggle("active");var e=this.nextElementSibling;e.style.display==="block"?e.style.display="none":e.style.display="block"})})</script></head><body><div id=app><div class=single-column-drawer-container id=drawer v-bind:class="{ 'single-column-drawer-container-active': isDrawerOpen }"><div class=drawer-content><div class=drawer-menu><a class="a-block drawer-menu-item false" href=/about>关于我
</a><a class="a-block drawer-menu-item active" href=/posts>归档
</a><a class="a-block drawer-menu-item false" href=/categories>分类
</a><a class="a-block drawer-menu-item false" href=/tags>标签
</a><a class="a-block drawer-menu-item false" href=/index.xml>订阅</a><div class=toc><div class=toc-content><center>目录</center><ul><ul><li><a href=#%e5%86%85%e5%ae%b9%e6%8f%90%e8%a6%81 class=nav-内容提要>内容提要</a></li><li><a href=#motivation class=nav-motivation>Motivation</a></li><li><a href=#%e6%a0%b8%e8%8c%83%e6%95%b0%e6%9c%80%e5%b0%8f%e5%8c%96 class=nav-核范数最小化>核范数最小化</a></li><ul><li><a href=#%e7%a7%a9%e6%9c%80%e5%b0%8f%e5%8c%96%e9%97%ae%e9%a2%98%e7%9a%84%e6%ad%a3%e5%bc%8f%e8%a1%a8%e8%bf%b0 class=nav-秩最小化问题的正式表述>秩最小化问题的正式表述</a></li><ul><li><a href=#%e4%bb%bf%e5%b0%84%e7%a7%a9%e6%9c%80%e5%b0%8f%e5%8c%96 class=nav-仿射秩最小化>仿射秩最小化</a></li></ul><li><a href=#convex-relaxation class=nav-convex-relaxation>Convex relaxation</a></li><li><a href=#%e4%b8%a4%e7%a7%8d%e9%97%ae%e9%a2%98%e7%9a%84%e7%ad%89%e4%bb%b7%e6%80%a7 class=nav-两种问题的等价性>两种问题的等价性</a></li><li><a href=#%e6%b1%82%e8%a7%a3%e6%a0%b8%e8%8c%83%e6%95%b0%e6%9c%80%e5%b0%8f%e5%8c%96%e9%97%ae%e9%a2%98%e7%9a%84%e7%ae%97%e6%b3%95 class=nav-求解核范数最小化问题的算法>求解核范数最小化问题的算法</a></li></ul><li><a href=#%e5%85%b6%e4%bb%96 class=nav-其他>其他</a></li><li><a href=#%e5%8f%82%e8%80%83 class=nav-参考>参考</a></li></ul></div></div></div></div></div><transition name=fade><div id=drawer-mask v-bind:class="{ 'single-column-drawer-mask': mounted }" v-if=isDrawerOpen v-on:click=toggleDrawer></div></transition><nav id=navBar class="navbar sticky-top navbar-light single-column-nav-container"><div id=navBackground class=nav-background></div><div class="container container-narrow nav-content"><button id=nav_dropdown_btn class=nav-dropdown-toggle type=button v-on:click=toggleDrawer>
<i class=material-icons>menu
</i></button>
<a id=navTitle class=navbar-brand href=https://gaxu.xyz/>嘉树的世界
</a><button type=button class=nav-darkmode-toggle id=darkModeToggleButton2>
<i class=material-icons id=darkModeToggleIcon2>dark_mode</i></button></div></nav><div class=single-column-header-container id=pageHead v-bind:style="{ transform: 'translateZ(0px) translateY('+.3*scrollY+'px)', opacity: 1-navOpacity }"><a href=https://gaxu.xyz/><div class=single-column-header-title>嘉树的世界</div><div class=single-column-header-subtitle>靡不有初，鲜克有终</div></a></div><div id=content><div id=streamContainer class=stream-container><div class="post-list-container post-list-container-shadow"><div class=post><div class=post-head-wrapper-text-only><div class=post-title>矩阵的秩、核范数与矩阵补全<div class=post-subtitle>矩阵核范数之于秩，一如向量 $\ell_1$ 范数之于 $\ell_0$ 范数。</div><div class=post-meta>嘉树 &nbsp;
<time itemprop=datePublished>2023-04-08 16:06
</time>&emsp;
<i class=material-icons>folder</i>
<a href=/categories/econometrics>Econometrics</a>
&nbsp;
<i class=material-icons>label</i>
<a href=/tags/%E7%9F%A9%E9%98%B5%E8%8C%83%E6%95%B0>矩阵范数</a></div></div></div><div class=post-body-wrapper><div class=post-body v-pre><div id=post-content><p>大约三周以前，听了一个presentation，讲到了 network 研究中度量关系网络的矩阵的 <em>「低秩」(low rank)</em> 性质。之后我在浏览一些高维计量文章时，又见到了这个概念，以及 <em>「核范数」(nuclear norm, aka trace norm)</em>。我印象里应该见过它们很多次，有些模糊的记忆片段，突然想起和硕士期间导师的交流中他提到过，遂从聊天记录中找到一些珍贵文件，当时他让我学习一下关于 <em>「矩阵补全」(matrix recovery, or matrix completion)</em> 的内容，我没来得及看。那么这几天就学习学习，说不定以后能派上用场。</p><h2 id=内容提要>内容提要&nbsp;<a class=hash-link href=#内容提要><i class="fas fa-link"></i></a></h2><p>本文将从矩阵恢复问题出发，介绍秩的最小化问题如何转变为一个核范数的最小化问题，前者是一个非凸的优化问题，很难求解，后者是一个凸优化问题，尽管不可求微分，但凸问题总是更易于求解的。</p><h2 id=motivation>Motivation&nbsp;<a class=hash-link href=#motivation><i class="fas fa-link"></i></a></h2><p>我们所感兴趣的问题是，如何基于非常有限的信息复原一个（低秩）矩阵。这一话题原本来自于工程实践的需要，例如计算机视觉领域的图像降噪，我们知道图像是由像素点构成的，因而整个图像可以视为一个大矩阵，当有一些像素被破坏，得到的就是一个不完整的矩阵，对图像降噪就是要补全那些污损或缺失的像素点；又如著名的 Netflix 问题（类似地，在国内语境下是豆瓣问题）：Netflix 用户评分矩阵只包含了某些用户对某些电影的评分，而不是每个用户对所有电影的评分（如下图所示），我们希望能推测出那些缺失的评分。这些问题都涉及到矩阵补全。</p><figure id=fig:ratings align role=group aria-describedby=caption-7a0da52ea67b6a781291fe53e8b9f6fe><a href=/posts/nuclear-norm-min/rating-mat.png class=img-link><img src=/posts/nuclear-norm-min/rating-mat.png style=width:60%></a><figcaption id=caption-7a0da52ea67b6a781291fe53e8b9f6fe>Rating Matrix (Source: Yuejie Chi, 2018, Lecture Notes)</figcaption></figure><p>那么，这和低秩又有什么关系呢？很显然，在没有任何限制条件下去补全那些未知的项，我们有无数种填充方式。想要唯一地重建一个矩阵，这个矩阵必须有些特殊的结构作为限制，而低秩就是一个合理的结构。低秩意味着，矩阵的列向量高度线性相关──它们扩展形成的线性空间是低维的。说得更加人话就是，数据中的大部分波动都由少数几个因子驱动。</p><p>现在可能又要问了，低秩在实践中是一个合理的假设吗？以 Netflix 评分矩阵为例，直觉上，同一类用户群体的行为、特征有很高的相似性，因此他们的潜在评分高度一致，这样评分矩阵的「主成分」实际上就是几个群体的评价，从而是低秩的。</p><h2 id=核范数最小化>核范数最小化&nbsp;<a class=hash-link href=#核范数最小化><i class="fas fa-link"></i></a></h2><p>低秩矩阵的补全可分为两类方法 (Gu <i>et al.</i>
2014)：<em>矩阵分解法</em> 和 <em>求解秩最小化问题法</em>。前者用两个低秩矩阵之积作为构造方式，可留待以后学习；后者是本文要讨论的内容。</p><h3 id=秩最小化问题的正式表述>秩最小化问题的正式表述&nbsp;<a class=hash-link href=#秩最小化问题的正式表述><i class="fas fa-link"></i></a></h3><p>假设真实矩阵是 $\bm{M}$，我们所观测到的元素集合是 $\{M_{i,j}\colon (i,j)\in\Omega\}$。秩最小化问题就表述为</p><p>$$
\min_{\bm{X}}\ \text{rank}(\bm{X}) \quad \text{s.t. } X_{i,j} = M_{i,j},\ \text{for every }(i,j)\in\Omega
$$</p><p>为方便起见，定义算子 $\mathcal{P}_{\Omega}$ 如下：</p><p>$$
[\mathcal{P}_{\Omega}(\bm{X})]_{i,j} = \begin{cases}
X_{i,j} & \text{if }(i,j)\in\Omega \\
0 & \text{otherwise}
\end{cases}
$$</p><p>换言之，$\mathcal{P}_{\Omega}$ 将任意矩阵正交投影到以指标集 $\Omega$ 作为支撑集的所有矩阵构成的空间上。</p><p>于是，上述问题就可以方便地写为</p><p>$$
\min_{\bm{X}}\ \text{rank}(\bm{X}) \quad \text{s.t. } \mathcal{P}_{\Omega}(\bm{X}) = \mathcal{P}_{\Omega}(\bm{M})
$$</p><p>问题的解就是我们对真实矩阵 $\bm{M}$ 的估计或者近似。</p><h4 id=仿射秩最小化>仿射秩最小化&nbsp;<a class=hash-link href=#仿射秩最小化><i class="fas fa-link"></i></a></h4><p>更一般地，我们不满足于普通的矩阵补全，而是以线性变换作为约束条件：</p><p>$$
\min_{\bm{X}}\ \text{rank}(\bm{X}) \quad \text{s.t. } \mathcal{A}(\bm{X}) = \bm{b}
$$</p><p>其中 $\mathcal{A}\colon\mathbb{R}^{m\times n}\to\mathbb{R}^p$ 是一个线性映射，向量 $\bm{b}\in\mathbb{R}^p$ 是给定的。这个问题被称为 <em>仿射秩最小化 (affine rank minimization)</em>。</p><style type=text/css>.notice{--root-color:#444;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#c33;--warning-content:#fee;--info-title:#fb7;--info-content:#fec;--note-title:#6be;--note-content:#e7f2fa;--tip-title:#5a5;--tip-content:#efe}@media(prefers-color-scheme:dark){.notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}}body.night .notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}.notice{padding:0 18px 18px;line-height:24px;margin-bottom:20px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:var(--title-color)}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative}</style><div><svg width="0" height="0" display="none"><symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379.0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628.0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628.0l-22.627 22.627c-6.248 6.248-6.248 16.379.0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"/></symbol><symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405.0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346 7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373.0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884.0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet"><path d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937.0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154.0l239.94 416.028zM288 354c-25.405.0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346 7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373.0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884.0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196.0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627.0 12 5.373 12 12v1e2h12c6.627.0 12 5.373 12 12v24z"/></symbol></svg></div><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>Note</p><p>矩阵空间上的线性变换本质上还是向量空间上的线性变换，$\mathcal{A}(\bm{X})$ 可以表述为
$$
\mathcal{A}(\bm{X}) := \begin{bmatrix}
\langle\bm{A}_1,\bm{X}\rangle_{\mathrm{F}} \\ \vdots \\ \langle\bm{A}_p,\bm{X}\rangle_{\mathrm{F}}
\end{bmatrix}
$$
其中 $\langle\bm{A},\bm{B}\rangle_{\mathrm{F}} = \text{trace}(\bm{A}'\bm{B})$ 是矩阵的 Frobenius 内积。见 <a href=https://math.stackexchange.com/questions/2592633/>Stack Exchange</a>。</p></div><p>不难证明，$\mathcal{P}_{\Omega}$ 是一个线性变换，因而 $\mathcal{P}_{\Omega}(\bm{X}) = \mathcal{P}_{\Omega}(\bm{M})$ 构成仿射约束的一个特例。</p><h3 id=convex-relaxation>Convex relaxation&nbsp;<a class=hash-link href=#convex-relaxation><i class="fas fa-link"></i></a></h3><p>遗憾的是，秩最小化问题不是一个凸优化问题，这不是我们所乐见的。原因是，秩函数（关于矩阵所有元素的函数）是非凸的。这无疑使得直接求解上述问题非常困难。我们希望找到作为替代的一个凸优化问题，它正是核范数最小化。</p><p>那么，什么是核范数呢？我们首先得了解 <em>奇异值分解 (singular value decomposition, SVD)</em>。任何一个矩阵 $\bm{X}\in\mathbb{R}^{m\times n}$，都可以作如下分解：</p><p>$$
\underset{(m\times n)}{\bm{X}} = \underset{(m\times r)}{\bm{U}}\,\underset{(r\times r)}{\bm{\Sigma}}\,\underset{(r\times n)}{\bm{V}&rsquo;}
$$</p><p>其中 $\bm{U}$ 和 $\bm{V}$ 各自的列是正交的：$\bm{U}'\bm{U}=\bm{I}_r=\bm{V}'\bm{V}$；$\bm{\Sigma}$ 是一个对角矩阵，对角线元素是从大到小排列的 <em>奇异值</em>：$\sigma_1\geq\sigma_2\geq\dots\geq\sigma_r\geq 0$，它们都是非负的；$r = \min\{m,n\}$。</p><p>矩阵的奇异值和秩紧密相关，事实上，一个矩阵的秩等于它所有非零（因而严格正）奇异值的个数，即</p><p>$$
\text{rank}(\bm{X}) = \sum_{i=1}^r\mathbb{1}\{\sigma_i\ne 0\}
$$</p><p>如果我们用 $\bm{\sigma}$ 来代表所有奇异值构成的向量，那么上式就是 $\bm{\sigma}$ 的 $\ell_0$ 范数。这里离题一下，向量 $\ell_0$ 范数并不是一个真正的范数，而是 pseudo norm，因为它没有齐次性，但实际应用中仍普遍称之为「范数」，见 <a href=https://en.wikipedia.org/wiki/Norm_(mathematics)#Hamming_distance_of_a_vector_from_zero>Wiki</a>。</p><p>现在，我们可以正式定义矩阵核范数：</p><div class=thm><p id=thm:nuclear class="thm-title thm-definition">Definition 1.</p><div class="thm-inner thm-definition">矩阵 $\bm{X}$ 的核范数 $\lVert\bm{X}\rVert_* := \sum_{i=1}^r \sigma_i(\bm{X})$，即所有奇异值之和。</div></div><p>可以看到，矩阵的核范数等于其奇异值向量的 $\ell_1$ 范数。核范数是一个真正的范数（要证明这一点，特别是要证明它满足三角不等式，见 <a href=https://math.stackexchange.com/questions/1142540>Stack Exchange 上的回答</a>）。</p><p>我们要做的替换就是将目标函数 $\text{rank}(\bm{X})$ 换为 $\lVert\bm{X}\rVert_*$，也就是我们考虑如下问题</p><p>$$
\min_{\bm{X}}\ \lVert\bm{X}\rVert_* \quad \text{s.t. } \mathcal{A}(\bm{X}) = \bm{b}
$$</p><p>这是一个凸优化问题，因为范数总是一个凸函数。核范数是秩函数的一个 <em>convex relaxation</em>。</p><div class="notice note"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#note-notice"/></svg></span>Note</p><p>可以类比 Lasso 的思想，想得到一个稀疏的向量（稀疏性以 $\ell_0$ 度量），用的是 $\ell_1$ 范数。现在是矩阵，度量矩阵列向量线性独立程度的正是秩（奇异值向量的 $\ell_0$ 范数），因而用核范数作为优化目标（奇异值向量的 $\ell_1$ 范数）可以得到低秩矩阵。</p></div><h3 id=两种问题的等价性>两种问题的等价性&nbsp;<a class=hash-link href=#两种问题的等价性><i class="fas fa-link"></i></a></h3><p>现在，我们如愿得到了一个凸优化问题，它比原问题更容易求解。但另一方面，这种替换是否有其正当性？换言之，最小化核范数的解是否就是最小化秩函数的解？如果不是，那显然背离了我们的初衷，我们当然希望两种问题给出的解是相同的。一般意义上，两种最小化当然不同，但在一些正则条件下可以证明它们的局部等价性，有诸多研究进行了论证。</p><p>Recht <i>et al.</i>
(2010) 证明了，在满足特定的 RIP (restricted isometry property) 条件时，仿射秩最小化问题等价于在相同约束下最小化核范数。Candès
and Recht
(2012) 则不依赖 RIP，证明了有很大的概率最小化核范数能唯一地复原矩阵，只要已知的元素个数不是太少。</p><p>在多数矩阵补全问题中，使用核范数最小化问题作为秩最小化问题的替代是恰当的。</p><h3 id=求解核范数最小化问题的算法>求解核范数最小化问题的算法&nbsp;<a class=hash-link href=#求解核范数最小化问题的算法><i class="fas fa-link"></i></a></h3><p>尽管核范数最小化问题是一个凸问题（但它仍是不可微分的），可以用一般性的凸优化工具求解，但求解速度可能并不十分理想。对此，有一些更迅速的方法，这里简单提及两种。</p><p>Recht <i>et al.</i>
(2010) 将核范数最小化问题转变为一个 <em>半定规划 (semidefinite programming)</em>，已有成熟的软件包用于解决这类问题（通常使用 primal-dual interior-point methods）。不过，这类方法在维度很高时速度很慢。</p><p>Cai <i>et al.</i>
(2010) 使用 <em>近端次梯度下降法 (proximal subgradient descent)</em> 求解，此法特别适用于不可微分的凸优化，而且对于高维情形收敛速度也可令人满意。以普通的矩阵补全为例，此文事实上是求解</p><p>$$
\min_{\bm{X}}\ \tau\lVert\bm{X}\rVert_* + \frac12\lVert\bm{X}\rVert_{\bm{F}}^2 \quad \text{s.t. } \mathcal{P}_{\Omega}(\bm{X}) = \mathcal{P}_{\Omega}(\bm{M})
$$</p><p>当 $\tau$ 很大时，该问题的解可以作为最小化核范数问题解的近似。求解该问题是通过一个渐进程序实现的，本质上是梯度下降，这里不做介绍。</p><h2 id=其他>其他&nbsp;<a class=hash-link href=#其他><i class="fas fa-link"></i></a></h2><p>Chandrasekaran <i>et al.</i>
(2012) 是一篇很好的文章，从几何角度揭示了范数的选择问题。具体到核范数，简单来说就是核范数是秩函数的凸包 （此概念在 <em><a href=../bayesian-persuasion>之前的文章</a></em> 中出现过），因而以核范数替代秩函数是 <em>最紧的凸放松 (tightest convex relaxation)</em>。</p><p>Gu <i>et al.</i>
(2014) 基于 Cai <i>et al.</i>
(2010) 的求解程序，研究了 <em>加权核范数最小化问题</em>，并设计了类似的求解程序。</p><h2 id=参考>参考&nbsp;<a class=hash-link href=#参考><i class="fas fa-link"></i></a></h2><p><a href=https://www.zhihu.com/question/26471536>知乎问答</a>.</p><p>Yuejie Chi (2018). <em><a href=https://users.ece.cmu.edu/~yuejiec/ece18898G_notes/ece18898g_lowrank_matrix_recovery.pdf>Lecture Notes</a>.</em></p><p>Yuxin Chen (2020). <em><a href=https://yuxinchen2020.github.io/ele520_math_data/lectures/matrix_recovery.pdf>Lecture Notes</a>.</em></p><div class=box><p>Cai, Jian-Feng <em>et al.</em>
(2010).
“A Singular Value Thresholding Algorithm for Matrix Completion.”
<i>SIAM Journal on Optimization</i>, 20(4), 1956–1982.
<a href=https://doi.org/10.1137/080738970 title="A Singular Value Thresholding Algorithm for Matrix Completion" target=_blank style=word-break:break-all>doi: 10.1137/080738970</a>.</p><p>Candès, Emmanuel and Recht, Benjamin
(2012).
“Exact matrix completion via convex optimization.”
<i>Communications of the ACM</i>, 55(6), 111–119.
<a href=https://doi.org/10.1145/2184319.2184343 title="Exact matrix completion via convex optimization" target=_blank style=word-break:break-all>doi: 10.1145/2184319.2184343</a>.</p><p>Chandrasekaran, Venkat <em>et al.</em>
(2012).
“The Convex Geometry of Linear Inverse Problems.”
<i>Foundations of Computational Mathematics</i>, 12(6), 805–849.
<a href=https://doi.org/10.1007/s10208-012-9135-7 title="The Convex Geometry of Linear Inverse Problems" target=_blank style=word-break:break-all>doi: 10.1007/s10208-012-9135-7</a>.</p><p>Gu, Shuhang <em>et al.</em>
(2014).
“Weighted Nuclear Norm Minimization with Application to Image Denoising.”
In: <em>2014 IEEE Conference on Computer Vision and Pattern Recognition</em>, Columbus, OH, USA.
<a href=https://doi.org/10.1109/CVPR.2014.366 title="Weighted Nuclear Norm Minimization with Application to Image Denoising" target=_blank style=word-break:break-all>doi: 10.1109/CVPR.2014.366</a>.</p><p>Gu, Shuhang <em>et al.</em>
(2017).
“Weighted Nuclear Norm Minimization and Its Applications to Low Level Vision.”
<i>International Journal of Computer Vision</i>, 121(2), 183–208.
<a href=https://doi.org/10.1007/s11263-016-0930-5 title="Weighted Nuclear Norm Minimization and Its Applications to Low Level Vision" target=_blank style=word-break:break-all>doi: 10.1007/s11263-016-0930-5</a>.</p><p>Recht, Benjamin <em>et al.</em>
(2010).
“Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization.”
<i>SIAM Review</i>, 52(3), 471–501.
<a href=https://doi.org/10.1137/070697835 title="Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization" target=_blank style=word-break:break-all>doi: 10.1137/070697835</a>.</p></div></div><div><hr width=100% id=EOF style="border-top:.6px solid #777;margin-top:2.5rem"><p style=color:#777>最后修改于 2023-04-11</p></div></div></div><nav class=post-pagination><a class=older-posts href=/posts/ridge-regression/>上回<br><em>Ridge Regression</em>
</a><a class=newer-posts href=/posts/ecmt-notes-stationarity/>下回<br><em>计量复习笔记 (I)：严格平稳性</em></a></nav><div class=post-comment-wrapper><div id=waline></div><script>Waline.init({el:"#waline",dark:"body.night",serverURL:"https://mysitecomments-2yzw3y3tk-cwleo.vercel.app"})</script></div></div></div></div></div><div id=sideContainer class=side-container><a class="a-block nav-head false" href=https://gaxu.xyz/><div class=nav-title>嘉树的世界</div><div class=nav-subtitle>靡不有初，鲜克有终</div></a><div class=nav-link-list><a class="a-block nav-link-item false" href=/about>关于我
</a><a class="a-block nav-link-item active" href=/posts>归档
</a><a class="a-block nav-link-item false" href=/categories>分类
</a><a class="a-block nav-link-item false" href=/tags>标签
</a><a class="a-block nav-link-item false" href=/index.xml>订阅</a></div><div class=nav-footer>由 <a href=https://gohugo.io>Hugo</a> 强力驱动<br>主题改动自 <a href=https://github.com/amazingrise/hugo-theme-diary>Diary</a><br>&copy;
2023–2024 嘉树.
本站遵循 <a href=https://creativecommons.org/licenses/by-nc/4.0/>CC-BY-NC 4.0</a> 协议</div></div><div id=extraContainer class=extra-container><div class=toc-wrapper><div class=toc><div class=toc-content><center>目录</center><ul><ul><li><a href=#%e5%86%85%e5%ae%b9%e6%8f%90%e8%a6%81 class=nav-内容提要>内容提要</a></li><li><a href=#motivation class=nav-motivation>Motivation</a></li><li><a href=#%e6%a0%b8%e8%8c%83%e6%95%b0%e6%9c%80%e5%b0%8f%e5%8c%96 class=nav-核范数最小化>核范数最小化</a></li><ul><li><a href=#%e7%a7%a9%e6%9c%80%e5%b0%8f%e5%8c%96%e9%97%ae%e9%a2%98%e7%9a%84%e6%ad%a3%e5%bc%8f%e8%a1%a8%e8%bf%b0 class=nav-秩最小化问题的正式表述>秩最小化问题的正式表述</a></li><ul><li><a href=#%e4%bb%bf%e5%b0%84%e7%a7%a9%e6%9c%80%e5%b0%8f%e5%8c%96 class=nav-仿射秩最小化>仿射秩最小化</a></li></ul><li><a href=#convex-relaxation class=nav-convex-relaxation>Convex relaxation</a></li><li><a href=#%e4%b8%a4%e7%a7%8d%e9%97%ae%e9%a2%98%e7%9a%84%e7%ad%89%e4%bb%b7%e6%80%a7 class=nav-两种问题的等价性>两种问题的等价性</a></li><li><a href=#%e6%b1%82%e8%a7%a3%e6%a0%b8%e8%8c%83%e6%95%b0%e6%9c%80%e5%b0%8f%e5%8c%96%e9%97%ae%e9%a2%98%e7%9a%84%e7%ae%97%e6%b3%95 class=nav-求解核范数最小化问题的算法>求解核范数最小化问题的算法</a></li></ul><li><a href=#%e5%85%b6%e4%bb%96 class=nav-其他>其他</a></li><li><a href=#%e5%8f%82%e8%80%83 class=nav-参考>参考</a></li></ul></div></div></div><div class=pagination><a id=globalBackToTop class="pagination-action animated-visibility" href=#top :class="{ invisible: scrollY == 0 }"><i class="material-icons pagination-action-icon">keyboard_arrow_up
</i></a><a type=button class=pagination-action id=darkModeToggleButton><span class="material-icons pagination-action-icon" id=darkModeToggleIcon>dark_mode</span></a></div></div><div id=single-column-footer>由 <a href=https://gohugo.io>Hugo</a> 强力驱动<br>主题改动自 <a href=https://github.com/amazingrise/hugo-theme-diary>Diary</a><br>&copy;
2023–2024 嘉树.
本站遵循 <a href=https://creativecommons.org/licenses/by-nc/4.0/>CC-BY-NC 4.0</a> 协议</div></div><script src=/js/journal.js></script></body></html>